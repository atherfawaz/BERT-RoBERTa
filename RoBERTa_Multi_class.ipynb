{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBERTa Multi-class.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtc4ba5EWgAoDSbzZCfsu/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0dcb761d5df1405d883cb134ea18c68c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ca24769caa0045d9aa99fe9b10da598d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fc73da9bb9fb4f61aa48ba2ce2f2baca",
              "IPY_MODEL_ef559a95bffe4542845eff228efcedf3"
            ]
          }
        },
        "ca24769caa0045d9aa99fe9b10da598d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fc73da9bb9fb4f61aa48ba2ce2f2baca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4a8263e9d49340c689bf0c724e36e3a2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_876c2601e410457d8fc2b4f6422b1dab"
          }
        },
        "ef559a95bffe4542845eff228efcedf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_67326d679e43401b94c14b22fd5630a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.29MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36d856b4156940dfbde3622940afb16a"
          }
        },
        "4a8263e9d49340c689bf0c724e36e3a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "876c2601e410457d8fc2b4f6422b1dab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67326d679e43401b94c14b22fd5630a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36d856b4156940dfbde3622940afb16a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atherfawaz/BERT-Supervised/blob/master/RoBERTa_Multi_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R43gl1TMysn",
        "colab_type": "text"
      },
      "source": [
        "# Supervised Learning with BERT\n",
        "\n",
        "***Question*: Supervised Learning with BERT\n",
        "The Astrological department believes that a person's astrological sign can be guessed from their behavior. An organization is collecting blog-posts of different people from various sources. You have been tasked to build a Deep Learning model that can use these posts data of individuals to predict which star group out of 12 does an individual belong to. You also need to tell the gender of that person.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orafObcOM7r0",
        "colab_type": "text"
      },
      "source": [
        "# Dataset\n",
        "The file name would contain the gender, age, occupation, and astrological sign of the blooger. For example, 4115891.male.24.Student.Leo.xml is one file. A single file will contain a set of blogs separated by date. To illustrate, this is what a sample file looks like:\n",
        "\n",
        "```\n",
        "<Blog>\n",
        "  <date>31,May,2004</date>\n",
        "    <post>\n",
        "      Well, everyone got up and going this morning.  It's still raining, but that's okay with me.  Sort of suits my mood.  I could easily have stayed home in bed with my book and the cats.  This has been a lot of rain though!..\n",
        "    </post>\n",
        "</Blog\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as3HYaN9NdGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a7b8ecc9-040c-484f-c4d8-730fe926ae43"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My Drive/Ebryx/blogs_train"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Ebryx/blogs_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjxcsMFHMntm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97e48ee3-e394-410c-d16e-155d383c4908"
      },
      "source": [
        "!pip install -q transformers\n",
        "!curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 778kB 3.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 24.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 34.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 47.5MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5115  100  5115    0     0  18010      0 --:--:-- --:--:-- --:--:-- 17947\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200515 ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Uninstalling torch-1.6.0+cu101:\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (49.6.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.12.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.6.20)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "Done updating TPU runtime\n",
            "  Successfully uninstalled torch-1.6.0+cu101\n",
            "Uninstalling torchvision-0.7.0+cu101:\n",
            "  Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][ 91.0 MiB/ 91.0 MiB]                                                \n",
            "Operation completed over 1 objects/91.0 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][119.5 MiB/119.5 MiB]                                                \n",
            "Operation completed over 1 objects/119.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.3 MiB/  2.3 MiB]                                                \n",
            "Operation completed over 1 objects/2.3 MiB.                                      \n",
            "Processing ./torch-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200515) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200515) (1.18.5)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.6.0a0+bf2bbd9\n",
            "Processing ./torch_xla-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+2b2085a\n",
            "Processing ./torchvision-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (7.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (1.6.0a0+bf2bbd9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200515) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.7.0a0+a6073f0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopenblas-dev is already the newest version (0.2.20+ds-4).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (385 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144579 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyjyqETWN_aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaTokenizerFast, RobertaForSequenceClassification\n",
        "from transformers import BertTokenizer, BertModel, BertConfig"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxPv8wSxOxq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TPU\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "from sklearn.utils import shuffle\n",
        "device = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFSvmrFvBDhT",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ybBFTYgBGin",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "0dcb761d5df1405d883cb134ea18c68c",
            "ca24769caa0045d9aa99fe9b10da598d",
            "fc73da9bb9fb4f61aa48ba2ce2f2baca",
            "ef559a95bffe4542845eff228efcedf3",
            "4a8263e9d49340c689bf0c724e36e3a2",
            "876c2601e410457d8fc2b4f6422b1dab",
            "67326d679e43401b94c14b22fd5630a7",
            "36d856b4156940dfbde3622940afb16a"
          ]
        },
        "outputId": "473111b5-5693-4542-ce57-06c988005f3e"
      },
      "source": [
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "#tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_size = 0.8"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dcb761d5df1405d883cb134ea18c68c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg0RDfSfNFFP",
        "colab_type": "text"
      },
      "source": [
        "# Parsing the dataset\n",
        "Parsing the dataset from separate files into a Pandas Dataframe for displaying and easy access. Some XML files contain encoding issues and the problematic contents of those files have been replaced by random number. While this could impact the accuracy of the model later, the effect would not be that big."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AwJThPGOgfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "bc9876c4-ef07-458d-e490-d985b09ad8b2"
      },
      "source": [
        "import re\n",
        "\n",
        "%cd /content/gdrive/My Drive/Ebryx/blogs_train\n",
        "df = pd.read_csv('/content/gdrive/My Drive/Ebryx/dataset.csv')\n",
        "df = df[['Post', 'Sign', 'Gender']]\n",
        "\n",
        "encode_dict = {}\n",
        "m_arr = np.identity(12, dtype=int)\n",
        "\n",
        "def encode_cat(x):\n",
        "  if x not in encode_dict.keys():\n",
        "    encode_dict[x]=len(encode_dict)\n",
        "  return encode_dict[x]\n",
        "\n",
        "def take_alphabets(x):\n",
        "  x = str(x).lower()\n",
        "  re.sub(r'\\W+', '', x)\n",
        "  re.sub('[^A-Za-z]+', '', x)\n",
        "  return x\n",
        "\n",
        "def one_hot(x):\n",
        "  return m_arr[x]\n",
        "\n",
        "df['Sign'] = df['Sign'].apply(lambda x: encode_cat(x))\n",
        "df['Post'] = df['Post'].str.replace('\\r','').str.replace('\\t','').str.replace('\\xa0', '').str.replace('\\n', '').str.replace('urlLink', '')\n",
        "df['Post'] = df['Post'].apply(lambda x: take_alphabets(x))\n",
        "df['Sign'] = df['Sign'].apply(lambda x: one_hot(x))\n",
        "\n",
        "df = df [['Post', 'Sign']]\n",
        "\n",
        "df = shuffle(df)\n",
        "df.head(10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Ebryx/blogs_train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Post</th>\n",
              "      <th>Sign</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>45461</th>\n",
              "      <td>had a gethering with my buddies.....</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269070</th>\n",
              "      <td>well considering that i am newly single...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358879</th>\n",
              "      <td>life is all about ass...either you...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2728</th>\n",
              "      <td>in my past lives i have been ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201278</th>\n",
              "      <td>ok so my mom randomly says yesterday \"h...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283994</th>\n",
              "      <td>glenn mrs solastri is droping...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81848</th>\n",
              "      <td>if you ever want to crash a system ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272644</th>\n",
              "      <td>oh, an interesting thing, i think.  my ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360597</th>\n",
              "      <td>sum artists i hate...or bands....  ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198981</th>\n",
              "      <td>i caught it the first time.  i did ...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     Post                                  Sign\n",
              "45461                had a gethering with my buddies.....  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
              "269070         well considering that i am newly single...  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
              "358879              life is all about ass...either you...  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
              "2728                     in my past lives i have been ...  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
              "201278         ok so my mom randomly says yesterday \"h...  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
              "283994                   glenn mrs solastri is droping...  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "81848              if you ever want to crash a system ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
              "272644         oh, an interesting thing, i think.  my ...  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
              "360597             sum artists i hate...or bands....  ...  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
              "198981             i caught it the first time.  i did ...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeURAytGVam6",
        "colab_type": "text"
      },
      "source": [
        "# Cleaning the dataset with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Egps6gVhEv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "f4e4a44c-3aea-4ea4-a8e0-e52b8b8dd835"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from progressbar import ProgressBar\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "pbar = ProgressBar()\n",
        "nltk.download('stopwords');\n",
        "nltk.download('punkt')\n",
        "stops = set(stopwords.words(\"english\"))  \n",
        "\n",
        "class preprocess():\n",
        "  def __init__(self, dataframe):\n",
        "    self.df = dataframe\n",
        "\n",
        "  def identify_tokens(self, row):\n",
        "    text = row['Post']\n",
        "    tokens = nltk.word_tokenize(str(text))\n",
        "    # taken only words (not punctuation)\n",
        "    token_words = [w for w in tokens if w.isalpha()]\n",
        "    return token_words\n",
        "  \n",
        "  def stem_list(self, row):\n",
        "    stemming = PorterStemmer()\n",
        "    my_list = row['words']\n",
        "    stemmed_list = [stemming.stem(word) for word in my_list]\n",
        "    return (stemmed_list)\n",
        "\n",
        "  def remove_stops(self, row):\n",
        "    my_list = row['stemmed_words']\n",
        "    meaningful_words = [w for w in my_list if not w in stops]\n",
        "    return (meaningful_words)\n",
        "  \n",
        "  def rejoin_words(self, row):\n",
        "    my_list = row['stem_meaningful']\n",
        "    joined_words = ( \" \".join(my_list))\n",
        "    return joined_words\n",
        "\n",
        "  def preprocess_dataframe(self):\n",
        "    print(\"Converting letters to lowercase...\")\n",
        "    self.df['Post'].str.lower()\n",
        "    print(\"Tokenizing sentences in dataset...\")\n",
        "    self.df['words'] = self.df.apply(self.identify_tokens, axis=1)\n",
        "    print(\"Stemming words in dataset...\")\n",
        "    self.df['stemmed_words'] = self.df.apply(self.stem_list, axis=1)\n",
        "    print(\"Removing stop words in dataset...\")\n",
        "    self.df['stem_meaningful'] = self.df.apply(self.remove_stops, axis=1)\n",
        "    self.df['Post'] = self.df.apply(self.rejoin_words, axis=1)\n",
        "    print(\"Pre-processing complete.\")\n",
        "    \n",
        "    return self.df\n",
        "\n",
        "df = preprocess(df).preprocess_dataframe()\n",
        "df = df[['Post', 'Sign']]\n",
        "\n",
        "#df.to_csv('/tmp/training_processed.csv', index=False)\n",
        "\n",
        "\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Converting letters to lowercase...\n",
            "Tokenizing sentences in dataset...\n",
            "Stemming words in dataset...\n",
            "Removing stop words in dataset...\n",
            "Pre-processing complete.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Post</th>\n",
              "      <th>Sign</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26014</th>\n",
              "      <td>whi I piss I went bed last night onli wake eno...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363023</th>\n",
              "      <td>player play like hack anoth poor perform playe...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16331</th>\n",
              "      <td>burn mail list forward thi Yo I onli wa friday...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184430</th>\n",
              "      <td>happi birthday nutter butter</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52267</th>\n",
              "      <td>woah back frm japan much fun wan na tok abt ti...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16634</th>\n",
              "      <td>AP report target new jersey soon attent wa su ...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191292</th>\n",
              "      <td>I play around blogger templat result see anyon...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171003</th>\n",
              "      <td>I love quizilla fraggl rock charact brought qu...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19610</th>\n",
              "      <td>I tire want feel faithless lost surfac I know ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2451</th>\n",
              "      <td>quick post befor work jami call around noon to...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     Post  Sign\n",
              "26014   whi I piss I went bed last night onli wake eno...     3\n",
              "363023  player play like hack anoth poor perform playe...     7\n",
              "16331   burn mail list forward thi Yo I onli wa friday...     6\n",
              "184430                       happi birthday nutter butter     0\n",
              "52267   woah back frm japan much fun wan na tok abt ti...     3\n",
              "16634   AP report target new jersey soon attent wa su ...     7\n",
              "191292  I play around blogger templat result see anyon...     9\n",
              "171003  I love quizilla fraggl rock charact brought qu...     9\n",
              "19610   I tire want feel faithless lost surfac I know ...     0\n",
              "2451    quick post befor work jami call around noon to...     8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORnJoe_4cpco",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "ed34249e-b13e-4bf8-cc15-6a98ec410a12"
      },
      "source": [
        "df.to_csv('/content/gdrive/My Drive/Ebryx/processed_dataset.csv', encoding='utf-8', index=False)\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Post</th>\n",
              "      <th>Sign</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26014</th>\n",
              "      <td>whi I piss I went bed last night onli wake eno...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363023</th>\n",
              "      <td>player play like hack anoth poor perform playe...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16331</th>\n",
              "      <td>burn mail list forward thi Yo I onli wa friday...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184430</th>\n",
              "      <td>happi birthday nutter butter</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52267</th>\n",
              "      <td>woah back frm japan much fun wan na tok abt ti...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16634</th>\n",
              "      <td>AP report target new jersey soon attent wa su ...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191292</th>\n",
              "      <td>I play around blogger templat result see anyon...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171003</th>\n",
              "      <td>I love quizilla fraggl rock charact brought qu...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19610</th>\n",
              "      <td>I tire want feel faithless lost surfac I know ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2451</th>\n",
              "      <td>quick post befor work jami call around noon to...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     Post  Sign\n",
              "26014   whi I piss I went bed last night onli wake eno...     3\n",
              "363023  player play like hack anoth poor perform playe...     7\n",
              "16331   burn mail list forward thi Yo I onli wa friday...     6\n",
              "184430                       happi birthday nutter butter     0\n",
              "52267   woah back frm japan much fun wan na tok abt ti...     3\n",
              "16634   AP report target new jersey soon attent wa su ...     7\n",
              "191292  I play around blogger templat result see anyon...     9\n",
              "171003  I love quizilla fraggl rock charact brought qu...     9\n",
              "19610   I tire want feel faithless lost surfac I know ...     0\n",
              "2451    quick post befor work jami call around noon to...     8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPYMZ0aNstC1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "d28e8d79-646f-4d44-af3c-8c1724e8d816"
      },
      "source": [
        "df = pd.read_csv('/content/gdrive/My Drive/Ebryx/processed_dataset.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Post</th>\n",
              "      <th>Sign</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>whi I piss I went bed last night onli wake eno...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>player play like hack anoth poor perform playe...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>burn mail list forward thi Yo I onli wa friday...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>happi birthday nutter butter</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>woah back frm japan much fun wan na tok abt ti...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Post  Sign\n",
              "0  whi I piss I went bed last night onli wake eno...     3\n",
              "1  player play like hack anoth poor perform playe...     7\n",
              "2  burn mail list forward thi Yo I onli wa friday...     6\n",
              "3                       happi birthday nutter butter     0\n",
              "4  woah back frm japan much fun wan na tok abt ti...     3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuHZfL61Ax7X",
        "colab_type": "text"
      },
      "source": [
        "# Creating a class for encapsulating the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_An5jwgLOr_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "  \n",
        "  def __init__(self, dataframe, tokenizer, max_len):\n",
        "      self.tokenizer = tokenizer\n",
        "      self.data = dataframe\n",
        "      self.post = dataframe.Post\n",
        "      self.targets = dataframe.Sign\n",
        "      self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.post)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      post = str(self.post[index])\n",
        "      post = \" \".join(post.split())\n",
        "\n",
        "      inputs = self.tokenizer.encode_plus(\n",
        "          post,\n",
        "          None,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_len,\n",
        "          truncation=True,\n",
        "          pad_to_max_length=True,\n",
        "          return_token_type_ids=True\n",
        "      )\n",
        "      ids = inputs['input_ids']\n",
        "      mask = inputs['attention_mask']\n",
        "      token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "      return {\n",
        "          'ids': torch.tensor(ids, dtype=torch.long),\n",
        "          'mask': torch.tensor(mask, dtype=torch.long),\n",
        "          'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "          'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_3csE--BROP",
        "colab_type": "text"
      },
      "source": [
        "# Separating into training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMlQffqIBQX6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "68177c90-5ce4-4dde-e0d2-7e84d494ec95"
      },
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset = df.sample(frac=train_size,random_state=200)\n",
        "test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (380720, 2)\n",
            "TRAIN Dataset: (304576, 2)\n",
            "TEST Dataset: (76144, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1NFa3NHQW9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = torch.nn.Linear(768, 12)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)\n",
        "\n",
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe5Wuic4NriW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "613e447d-a4c3-466a-b064-1e4109d09314"
      },
      "source": [
        "def train(epoch):\n",
        "  model.train()\n",
        "  for i,data in enumerate(training_loader, 0):\n",
        "    ids = data['ids'].to(device, dtype = torch.long)\n",
        "    mask = data['mask'].to(device, dtype = torch.long)\n",
        "    token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "    targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "    outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    if i%100==0:\n",
        "        print(f'[ {i} ] Loss: {loss.item()}')\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #optimizer.step()\n",
        "    xm.optimizer_step(optimizer)\n",
        "    xm.mark_step()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  train(epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0 ] Loss: 0.7125088572502136\n",
            "[ 100 ] Loss: 0.31261903047561646\n",
            "[ 200 ] Loss: 0.29223331809043884\n",
            "[ 300 ] Loss: 0.28564220666885376\n",
            "[ 400 ] Loss: 0.2900067865848541\n",
            "[ 500 ] Loss: 0.27668482065200806\n",
            "[ 600 ] Loss: 0.2913244366645813\n",
            "[ 700 ] Loss: 0.28977906703948975\n",
            "[ 800 ] Loss: 0.2860506474971771\n",
            "[ 900 ] Loss: 0.2965611219406128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-ChckoKKPFk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Epoch 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vQzyNLWRX6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = ModelClass()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "model.to(device)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  steps = 0\n",
        "  n_tr_correct, n_val_correct = 0, 0\n",
        "  tr_loss, val_loss = 0, 0\n",
        "  nb_tr_examples, nb_val_examples = 0, 0\n",
        "  for (i, data), (j, val_data) in zip(enumerate(training_loader, 0), enumerate(validation_loader, 0)):\n",
        "      model.train()\n",
        "      ids = data['ids'].to(device, dtype = torch.long)\n",
        "      mask = data['mask'].to(device, dtype = torch.long)\n",
        "      targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "      outputs = model(ids, mask).squeeze()\n",
        "      optimizer.zero_grad()\n",
        "      loss = loss_function(outputs, targets)\n",
        "      big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "      \n",
        "      n_tr_correct += calcuate_accu(big_idx, targets)\n",
        "      nb_tr_examples += targets.size(0)\n",
        "\n",
        "      tr_loss += loss.item()\n",
        "      \n",
        "      if i%100==0:\n",
        "        loss_step = tr_loss/100\n",
        "        train_acc = n_tr_correct/nb_tr_examples\n",
        "        tr_loss, n_tr_correct, nb_tr_examples = 0, 0, 0\n",
        "        print(f\"[{i}] train_loss: {loss_step:.3f} train_acc: {train_acc:.3f}\", end=\"    \") \n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      \n",
        "      # # When using GPU or GPU\n",
        "      # optimizer.step()\n",
        "      \n",
        "      # When using TPU\n",
        "      xm.optimizer_step(optimizer)\n",
        "      xm.mark_step()\n",
        "\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          ids = val_data['ids'].to(device, dtype = torch.long)\n",
        "          mask = val_data['mask'].to(device, dtype = torch.long)\n",
        "          targets = val_data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "          outputs = model(ids, mask).squeeze()\n",
        "          loss = loss_function(outputs, targets)\n",
        "          big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "          \n",
        "          n_val_correct += calcuate_accu(big_idx, targets)\n",
        "          nb_val_examples += targets.size(0)\n",
        "\n",
        "          val_loss += loss.item()\n",
        "          \n",
        "          if j%100==0:\n",
        "            val_step = val_loss/100\n",
        "            val_acc = n_val_correct/nb_val_examples\n",
        "            val_loss, n_val_correct, nb_val_examples = 0, 0, 0\n",
        "            print(f\"val_loss: {val_step:.3f} val_acc : {val_acc:.3f}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlJRssznrcs2",
        "colab_type": "text"
      },
      "source": [
        "# Saving model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aX8WmwsKV2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = '/content/gdrive/My Drive/Ebryx/roberta_12class.pt'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8seU-V9rlY7",
        "colab_type": "text"
      },
      "source": [
        "# Loading trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E04wAxzrhZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e2367a3-cdb0-4b39-d7e4-2416ab00e331"
      },
      "source": [
        "PATH = '/content/gdrive/My Drive/Ebryx/roberta_12class.pt'\n",
        "model = ModelClass()\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "#model.eval()\n",
        "#print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNXSP9cfKIz2",
        "colab_type": "text"
      },
      "source": [
        "# Epoch 2-N"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LaTJNQ1KBKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c218f03-597c-4b87-8427-b36e0287be39"
      },
      "source": [
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "model.to(device)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "steps = 0\n",
        "n_tr_correct, n_val_correct = 0, 0\n",
        "tr_loss, val_loss = 0, 0\n",
        "nb_tr_examples, nb_val_examples = 0, 0\n",
        "for (i, data), (j, val_data) in zip(enumerate(training_loader, 0), enumerate(validation_loader, 0)):\n",
        "    model.train()\n",
        "    ids = data['ids'].to(device, dtype = torch.long)\n",
        "    mask = data['mask'].to(device, dtype = torch.long)\n",
        "    targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "    outputs = model(ids, mask).squeeze()\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_function(outputs, targets)\n",
        "    big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "    \n",
        "    n_tr_correct += calcuate_accu(big_idx, targets)\n",
        "    nb_tr_examples += targets.size(0)\n",
        "\n",
        "    tr_loss += loss.item()\n",
        "    \n",
        "    if i%100==0:\n",
        "      loss_step = tr_loss/100\n",
        "      train_acc = n_tr_correct/nb_tr_examples\n",
        "      tr_loss, n_tr_correct, nb_tr_examples = 0, 0, 0\n",
        "      print(f\"[{i}] train_loss: {loss_step:.3f} train_acc: {train_acc:.3f}\", end=\"    \") \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    \n",
        "    # # When using GPU or GPU\n",
        "    # optimizer.step()\n",
        "    \n",
        "    # When using TPU\n",
        "    xm.optimizer_step(optimizer)\n",
        "    xm.mark_step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        ids = val_data['ids'].to(device, dtype = torch.long)\n",
        "        mask = val_data['mask'].to(device, dtype = torch.long)\n",
        "        targets = val_data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask).squeeze()\n",
        "        loss = loss_function(outputs, targets)\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        \n",
        "        n_val_correct += calcuate_accu(big_idx, targets)\n",
        "        nb_val_examples += targets.size(0)\n",
        "\n",
        "        val_loss += loss.item()\n",
        "        \n",
        "        if j%100==0:\n",
        "          val_step = val_loss/100\n",
        "          val_acc = n_val_correct/nb_val_examples\n",
        "          val_loss, n_val_correct, nb_val_examples = 0, 0, 0\n",
        "          print(f\"val_loss: {val_step:.3f} val_acc : {val_acc:.3f}\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0] train_loss: 0.018 train_acc: 0.375    val_loss: 0.020 val_acc : 0.250\n",
            "[100] train_loss: 2.022 train_acc: 0.308    val_loss: 2.012 val_acc : 0.306\n",
            "[200] train_loss: 2.034 train_acc: 0.322    val_loss: 2.008 val_acc : 0.310\n",
            "[300] train_loss: 2.006 train_acc: 0.323    val_loss: 1.973 val_acc : 0.334\n",
            "[400] train_loss: 2.005 train_acc: 0.319    val_loss: 1.967 val_acc : 0.323\n",
            "[500] train_loss: 2.032 train_acc: 0.309    val_loss: 2.011 val_acc : 0.331\n",
            "[600] train_loss: 2.006 train_acc: 0.319    val_loss: 1.906 val_acc : 0.371\n",
            "[700] train_loss: 2.035 train_acc: 0.304    val_loss: 2.020 val_acc : 0.335\n",
            "[800] train_loss: 1.996 train_acc: 0.321    val_loss: 1.947 val_acc : 0.346\n",
            "[900] train_loss: 2.030 train_acc: 0.318    val_loss: 1.965 val_acc : 0.344\n",
            "[1000] train_loss: 2.039 train_acc: 0.314    val_loss: 1.976 val_acc : 0.321\n",
            "[1100] train_loss: 2.009 train_acc: 0.309    val_loss: 1.942 val_acc : 0.340\n",
            "[1200] train_loss: 2.023 train_acc: 0.315    val_loss: 1.932 val_acc : 0.344\n",
            "[1300] train_loss: 2.005 train_acc: 0.320    val_loss: 2.019 val_acc : 0.334\n",
            "[1400] train_loss: 2.032 train_acc: 0.309    val_loss: 1.995 val_acc : 0.341\n",
            "[1500] train_loss: 2.024 train_acc: 0.319    val_loss: 2.033 val_acc : 0.325\n",
            "[1600] train_loss: 2.004 train_acc: 0.319    val_loss: 1.939 val_acc : 0.367\n",
            "[1700] train_loss: 2.013 train_acc: 0.320    val_loss: 1.959 val_acc : 0.340\n",
            "[1800] train_loss: 2.043 train_acc: 0.312    val_loss: 1.967 val_acc : 0.336\n",
            "[1900] train_loss: 2.001 train_acc: 0.322    val_loss: 1.886 val_acc : 0.369\n",
            "[2000] train_loss: 2.024 train_acc: 0.314    val_loss: 1.995 val_acc : 0.325\n",
            "[2100] train_loss: 2.029 train_acc: 0.318    val_loss: 1.973 val_acc : 0.343\n",
            "[2200] train_loss: 2.013 train_acc: 0.321    val_loss: 1.922 val_acc : 0.335\n",
            "[2300] train_loss: 2.029 train_acc: 0.313    val_loss: 2.003 val_acc : 0.324\n",
            "[2400] train_loss: 2.014 train_acc: 0.323    val_loss: 1.963 val_acc : 0.360\n",
            "[2500] train_loss: 2.024 train_acc: 0.317    val_loss: 1.934 val_acc : 0.344\n",
            "[2600] train_loss: 2.001 train_acc: 0.325    val_loss: 1.976 val_acc : 0.338\n",
            "[2700] train_loss: 2.015 train_acc: 0.320    val_loss: 1.976 val_acc : 0.318\n",
            "[2800] train_loss: 2.019 train_acc: 0.319    val_loss: 1.931 val_acc : 0.359\n",
            "[2900] train_loss: 2.046 train_acc: 0.301    val_loss: 1.945 val_acc : 0.339\n",
            "[3000] train_loss: 2.018 train_acc: 0.313    val_loss: 1.963 val_acc : 0.334\n",
            "[3100] train_loss: 2.020 train_acc: 0.318    val_loss: 1.926 val_acc : 0.321\n",
            "[3200] train_loss: 2.003 train_acc: 0.321    val_loss: 1.917 val_acc : 0.336\n",
            "[3300] train_loss: 2.023 train_acc: 0.321    val_loss: 1.976 val_acc : 0.336\n",
            "[3400] train_loss: 2.031 train_acc: 0.321    val_loss: 1.946 val_acc : 0.355\n",
            "[3500] train_loss: 2.041 train_acc: 0.302    val_loss: 1.997 val_acc : 0.325\n",
            "[3600] train_loss: 2.038 train_acc: 0.315    val_loss: 1.955 val_acc : 0.334\n",
            "[3700] train_loss: 2.043 train_acc: 0.310    val_loss: 1.988 val_acc : 0.340\n",
            "[3800] train_loss: 2.034 train_acc: 0.313    val_loss: 1.940 val_acc : 0.364\n",
            "[3900] train_loss: 2.001 train_acc: 0.323    val_loss: 1.962 val_acc : 0.343\n",
            "[4000] train_loss: 2.028 train_acc: 0.302    val_loss: 1.959 val_acc : 0.344\n",
            "[4100] train_loss: 2.003 train_acc: 0.335    val_loss: 1.978 val_acc : 0.336\n",
            "[4200] train_loss: 2.027 train_acc: 0.315    val_loss: 1.949 val_acc : 0.345\n",
            "[4300] train_loss: 2.021 train_acc: 0.326    val_loss: 1.999 val_acc : 0.324\n",
            "[4400] train_loss: 1.998 train_acc: 0.334    val_loss: 1.938 val_acc : 0.334\n",
            "[4500] train_loss: 2.008 train_acc: 0.328    val_loss: 1.934 val_acc : 0.357\n",
            "[4600] train_loss: 2.007 train_acc: 0.324    val_loss: 1.961 val_acc : 0.345\n",
            "[4700] train_loss: 2.016 train_acc: 0.314    val_loss: 1.928 val_acc : 0.343\n",
            "[4800] train_loss: 2.064 train_acc: 0.306    val_loss: 2.032 val_acc : 0.292\n",
            "[4900] train_loss: 2.017 train_acc: 0.318    val_loss: 2.016 val_acc : 0.318\n",
            "[5000] train_loss: 2.016 train_acc: 0.322    val_loss: 1.977 val_acc : 0.305\n",
            "[5100] train_loss: 2.018 train_acc: 0.314    val_loss: 1.934 val_acc : 0.349\n",
            "[5200] train_loss: 1.989 train_acc: 0.326    val_loss: 1.944 val_acc : 0.331\n",
            "[5300] train_loss: 2.019 train_acc: 0.322    val_loss: 1.959 val_acc : 0.350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c84133e70aed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKJ0M6LXJGpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = '/content/gdrive/My Drive/Ebryx/roberta_12class.pt'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}