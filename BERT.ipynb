{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1uedKqXll7IbJTxxtdX82ESNvP1Zy9vWa",
      "authorship_tag": "ABX9TyPoyPzAdG2DeRjTlemqF24H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atherfawaz/BERT-Supervised/blob/master/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca7jIEhZNU_Q",
        "colab_type": "text"
      },
      "source": [
        "# Supervised Learning with BERT\n",
        "\n",
        "***Question*: Supervised Learning with BERT\n",
        "The Astrological department believes that a person's astrological sign can be guessed from their behavior. An organization is collecting blog-posts of different people from various sources. You have been tasked to build a Deep Learning model that can use these posts data of individuals to predict which star group out of 12 does an individual belong to. You also need to tell the gender of that person.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PIs-jEzMCT4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b40a55fb-d7f9-4803-a413-837a9a48e0ae"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "%cd /content/drive/My Drive/Ebryx/blogs_train"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Ebryx/blogs_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GW1LBluMU3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xvf  'blogs_train.tar.xz'\n",
        "%cd /content/drive/My Drive/Ebryx/blogs_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ArTrha5U1RA",
        "colab_type": "text"
      },
      "source": [
        "# Dataset\n",
        "The file name would contain the gender, age, occupation, and astrological sign of the blooger. For example, 4115891.male.24.Student.Leo.xml is one file. A single file will contain a set of blogs separated by date. To illustrate, this is what a sample file looks like:\n",
        "\n",
        "```\n",
        "<Blog>\n",
        "  <date>31,May,2004</date>\n",
        "    <post>\n",
        "      Well, everyone got up and going this morning.  It's still raining, but that's okay with me.  Sort of suits my mood.  I could easily have stayed home in bed with my book and the cats.  This has been a lot of rain though!..\n",
        "    </post>\n",
        "</Blog\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN1hRWkRd6vI",
        "colab_type": "text"
      },
      "source": [
        "# Parsing the dataset\n",
        "Parsing the dataset from separate files into a Pandas Dataframe for displaying and easy access. Some XML files contain encoding issues and the problematic contents of those files have been replaced by random number. While this could impact the accuracy of the model later, the effect would not be that big."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYh3rv4SnJ1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import codecs\n",
        "from bs4 import BeautifulSoup\n",
        "from progressbar import ProgressBar\n",
        "pbar = ProgressBar()\n",
        "\n",
        "print('PARSING FILES....')\n",
        "\n",
        "FILES = os.listdir()\n",
        "#print('File count: ', len(FILES))\n",
        "#print(FILES)\n",
        "#FILES = ['4115891.male.24.Student.Leo.xml', '4115958.male.16.Communications-Media.Libra.xml', '4116071.male.26.Arts.Sagittarius.xml', '4116243.female.24.Manufacturing.Sagittarius.xml']\n",
        "\n",
        "posts_arr = []\n",
        "sign_arr = []\n",
        "gender_arr = []\n",
        "age_arr = []\n",
        "occupation_arr = []\n",
        "\n",
        "for to_fetch in pbar(FILES):\n",
        "    #print('Parsing file:', to_fetch)\n",
        "    gender = to_fetch.split('.')[-5]\n",
        "    age = to_fetch.split('.')[-4]\n",
        "    occupation = to_fetch.split('.')[-3]\n",
        "    sign = to_fetch.split('.')[-2]\n",
        "    with codecs.open(to_fetch, 'r', encoding='utf-8', errors='ignore') as fp:\n",
        "      soup = BeautifulSoup(fp, 'lxml',\n",
        "                           from_encoding='utf8')\n",
        "      posts = soup.find_all('post')\n",
        "      #print(soup.prettify())\n",
        "      for post in posts:\n",
        "        clean_str = post.text\n",
        "        clean_str = clean_str.replace('\\r', '')\n",
        "        clean_str = clean_str.replace('\\n', '')\n",
        "        posts_arr.append(clean_str)\n",
        "        sign_arr.append(sign)\n",
        "        age_arr.append(age)\n",
        "        occupation_arr.append(occupation)\n",
        "        gender_arr.append(gender)\n",
        "\n",
        "df = pd.DataFrame({'Gender': gender_arr, 'Age': age_arr, 'Occupation': occupation_arr, 'Post': posts_arr, 'Sign': sign_arr})\n",
        "\n",
        "#df.head(50)\n",
        "df.to_csv('/content/drive/My Drive/Ebryx/dataset.csv', encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjGpg5ZfnmcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knjTjaAtcx-H",
        "colab_type": "text"
      },
      "source": [
        "# Loading DistilledBERT for fine tuning\n",
        "\n",
        "Taking help from the model implementation from huggingface and [this repository](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb#scrollTo=JrBr2YesGdO_).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKyIOfh5jmCz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "08690878-faad-447f-fbfa-163d07b5756b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 8.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 30.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 55.5MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 51.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=4c027cb993a7f56d004666232246d01eaedd5793086079be1560343c8da64299\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32pVsuaBj12B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "2f7d99bb-57a4-469d-e664-e2fe306d4708"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Aug 18 04:51:24 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7amTtuAjhZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b862db62-ae1e-4165-a76c-a553e482ca52"
      },
      "source": [
        "# Importing the libraries needed\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 0.01\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
        "\n",
        "# Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBPNbR2tl8sZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "1a902ed8-e536-4777-da13-5f810fda56a8"
      },
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "df = pd.read_csv('/content/drive/My Drive/Ebryx/dataset.csv')\n",
        "df = df[['Post', 'Sign']]\n",
        "\n",
        "encode_dict = {}\n",
        "\n",
        "def encode_cat(x):\n",
        "    if x not in encode_dict.keys():\n",
        "        encode_dict[x]=len(encode_dict)\n",
        "    return encode_dict[x]\n",
        "\n",
        "df['Sign'] = df['Sign'].apply(lambda x: encode_cat(x))\n",
        "\n",
        "df.head(100)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Post</th>\n",
              "      <th>Sign</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I just watched Beauty and the beast...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This picture shows a Vietnamese ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So I just used the term “Bad Ass...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This is a dumb little story I whipp...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Listen, you fuckers, you screwhe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>\\t         There is a reason that I haven't be...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>\\t         After I wrote that last bit, I prom...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>\\t         Today is just not my day... I break...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>\\t         Last night Grandpa got me and asked...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>\\t         Andrew's going away and I'm sad, be...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Post  Sign\n",
              "0              I just watched Beauty and the beast...     0\n",
              "1                 This picture shows a Vietnamese ...     0\n",
              "2                 So I just used the term “Bad Ass...     0\n",
              "3              This is a dumb little story I whipp...     0\n",
              "4                 Listen, you fuckers, you screwhe...     0\n",
              "..                                                ...   ...\n",
              "95  \\t         There is a reason that I haven't be...     6\n",
              "96  \\t         After I wrote that last bit, I prom...     6\n",
              "97  \\t         Today is just not my day... I break...     6\n",
              "98  \\t         Last night Grandpa got me and asked...     6\n",
              "99  \\t         Andrew's going away and I'm sad, be...     6\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT4MFsu3kW8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "eaeafb90-405d-4a87-de7b-0981f6f25301"
      },
      "source": [
        "class Triage(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        post = str(self.data.Post[index])\n",
        "        post = \" \".join(post.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            post,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.data.Sign[index], dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state=200)\n",
        "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (380720, 2)\n",
            "TRAIN Dataset: (304576, 2)\n",
            "TEST Dataset: (76144, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNFZh2WQkhl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB0uZ-dgnJm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class DistillBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistillBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 12)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "model = DistillBERTClass()\n",
        "model.to(device)\n",
        "\n",
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwhi-s_AnR4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0119649b-18fb-4bc4-abac-5cbf0e467be4"
      },
      "source": [
        "# Function to calcuate the accuracy of the model\n",
        "\n",
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct\n",
        "\n",
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for i,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if i%5 == 0:\n",
        "          loss_step = tr_loss/nb_tr_steps\n",
        "          accu_step = (n_correct*100)/nb_tr_examples \n",
        "          print(f\"[ {i} ] Training Loss {loss_step:.3f}---Training Accuracy: {accu_step:.3f}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return \n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0 ] Training Loss 2.487---Training Accuracy: 6.250\n",
            "[ 5 ] Training Loss 9.637---Training Accuracy: 6.250\n",
            "[ 10 ] Training Loss 7.684---Training Accuracy: 7.102\n",
            "[ 15 ] Training Loss 6.122---Training Accuracy: 7.812\n",
            "[ 20 ] Training Loss 5.264---Training Accuracy: 8.185\n",
            "[ 25 ] Training Loss 4.742---Training Accuracy: 7.572\n",
            "[ 30 ] Training Loss 4.379---Training Accuracy: 7.964\n",
            "[ 35 ] Training Loss 4.115---Training Accuracy: 8.420\n",
            "[ 40 ] Training Loss 3.917---Training Accuracy: 8.308\n",
            "[ 45 ] Training Loss 3.762---Training Accuracy: 8.152\n",
            "[ 50 ] Training Loss 3.636---Training Accuracy: 8.333\n",
            "[ 55 ] Training Loss 3.532---Training Accuracy: 8.315\n",
            "[ 60 ] Training Loss 3.445---Training Accuracy: 8.607\n",
            "[ 65 ] Training Loss 3.372---Training Accuracy: 8.807\n",
            "[ 70 ] Training Loss 3.309---Training Accuracy: 8.803\n",
            "[ 75 ] Training Loss 3.254---Training Accuracy: 8.840\n",
            "[ 80 ] Training Loss 3.206---Training Accuracy: 9.144\n",
            "[ 85 ] Training Loss 3.163---Training Accuracy: 9.375\n",
            "[ 90 ] Training Loss 3.126---Training Accuracy: 9.444\n",
            "[ 95 ] Training Loss 3.091---Training Accuracy: 9.798\n",
            "[ 100 ] Training Loss 3.060---Training Accuracy: 9.963\n",
            "[ 105 ] Training Loss 3.033---Training Accuracy: 10.053\n",
            "[ 110 ] Training Loss 3.008---Training Accuracy: 9.882\n",
            "[ 115 ] Training Loss 2.985---Training Accuracy: 9.914\n",
            "[ 120 ] Training Loss 2.963---Training Accuracy: 9.995\n",
            "[ 125 ] Training Loss 2.944---Training Accuracy: 9.995\n",
            "[ 130 ] Training Loss 2.926---Training Accuracy: 10.091\n",
            "[ 135 ] Training Loss 2.909---Training Accuracy: 10.041\n",
            "[ 140 ] Training Loss 2.894---Training Accuracy: 10.084\n",
            "[ 145 ] Training Loss 2.879---Training Accuracy: 10.210\n",
            "[ 150 ] Training Loss 2.865---Training Accuracy: 10.389\n",
            "[ 155 ] Training Loss 2.853---Training Accuracy: 10.317\n",
            "[ 160 ] Training Loss 2.841---Training Accuracy: 10.423\n",
            "[ 165 ] Training Loss 2.831---Training Accuracy: 10.392\n",
            "[ 170 ] Training Loss 2.820---Training Accuracy: 10.344\n",
            "[ 175 ] Training Loss 2.811---Training Accuracy: 10.369\n",
            "[ 180 ] Training Loss 2.802---Training Accuracy: 10.359\n",
            "[ 185 ] Training Loss 2.793---Training Accuracy: 10.349\n",
            "[ 190 ] Training Loss 2.785---Training Accuracy: 10.324\n",
            "[ 195 ] Training Loss 2.778---Training Accuracy: 10.284\n",
            "[ 200 ] Training Loss 2.770---Training Accuracy: 10.230\n",
            "[ 205 ] Training Loss 2.763---Training Accuracy: 10.225\n",
            "[ 210 ] Training Loss 2.757---Training Accuracy: 10.338\n",
            "[ 215 ] Training Loss 2.750---Training Accuracy: 10.330\n",
            "[ 220 ] Training Loss 2.743---Training Accuracy: 10.365\n",
            "[ 225 ] Training Loss 2.737---Training Accuracy: 10.315\n",
            "[ 230 ] Training Loss 2.731---Training Accuracy: 10.444\n",
            "[ 235 ] Training Loss 2.726---Training Accuracy: 10.395\n",
            "[ 240 ] Training Loss 2.721---Training Accuracy: 10.335\n",
            "[ 245 ] Training Loss 2.716---Training Accuracy: 10.366\n",
            "[ 250 ] Training Loss 2.711---Training Accuracy: 10.334\n",
            "[ 255 ] Training Loss 2.706---Training Accuracy: 10.400\n",
            "[ 260 ] Training Loss 2.702---Training Accuracy: 10.393\n",
            "[ 265 ] Training Loss 2.698---Training Accuracy: 10.362\n",
            "[ 270 ] Training Loss 2.694---Training Accuracy: 10.286\n",
            "[ 275 ] Training Loss 2.690---Training Accuracy: 10.258\n",
            "[ 280 ] Training Loss 2.687---Training Accuracy: 10.165\n",
            "[ 285 ] Training Loss 2.683---Training Accuracy: 10.140\n",
            "[ 290 ] Training Loss 2.679---Training Accuracy: 10.148\n",
            "[ 295 ] Training Loss 2.676---Training Accuracy: 10.167\n",
            "[ 300 ] Training Loss 2.672---Training Accuracy: 10.174\n",
            "[ 305 ] Training Loss 2.669---Training Accuracy: 10.161\n",
            "[ 310 ] Training Loss 2.666---Training Accuracy: 10.229\n",
            "[ 315 ] Training Loss 2.663---Training Accuracy: 10.196\n",
            "[ 320 ] Training Loss 2.660---Training Accuracy: 10.105\n",
            "[ 325 ] Training Loss 2.657---Training Accuracy: 10.113\n",
            "[ 330 ] Training Loss 2.655---Training Accuracy: 10.159\n",
            "[ 335 ] Training Loss 2.652---Training Accuracy: 10.156\n",
            "[ 340 ] Training Loss 2.649---Training Accuracy: 10.191\n",
            "[ 345 ] Training Loss 2.647---Training Accuracy: 10.143\n",
            "[ 350 ] Training Loss 2.644---Training Accuracy: 10.087\n",
            "[ 355 ] Training Loss 2.642---Training Accuracy: 10.077\n",
            "[ 360 ] Training Loss 2.639---Training Accuracy: 10.085\n",
            "[ 365 ] Training Loss 2.637---Training Accuracy: 10.015\n",
            "[ 370 ] Training Loss 2.635---Training Accuracy: 9.973\n",
            "[ 375 ] Training Loss 2.633---Training Accuracy: 9.957\n",
            "[ 380 ] Training Loss 2.631---Training Accuracy: 9.957\n",
            "[ 385 ] Training Loss 2.629---Training Accuracy: 9.950\n",
            "[ 390 ] Training Loss 2.627---Training Accuracy: 9.910\n",
            "[ 395 ] Training Loss 2.625---Training Accuracy: 9.864\n",
            "[ 400 ] Training Loss 2.623---Training Accuracy: 9.811\n",
            "[ 405 ] Training Loss 2.622---Training Accuracy: 9.783\n",
            "[ 410 ] Training Loss 2.620---Training Accuracy: 9.786\n",
            "[ 415 ] Training Loss 2.618---Training Accuracy: 9.751\n",
            "[ 420 ] Training Loss 2.617---Training Accuracy: 9.709\n",
            "[ 425 ] Training Loss 2.615---Training Accuracy: 9.756\n",
            "[ 430 ] Training Loss 2.613---Training Accuracy: 9.796\n",
            "[ 435 ] Training Loss 2.612---Training Accuracy: 9.791\n",
            "[ 440 ] Training Loss 2.610---Training Accuracy: 9.772\n",
            "[ 445 ] Training Loss 2.609---Training Accuracy: 9.718\n",
            "[ 450 ] Training Loss 2.608---Training Accuracy: 9.687\n",
            "[ 455 ] Training Loss 2.606---Training Accuracy: 9.677\n",
            "[ 460 ] Training Loss 2.605---Training Accuracy: 9.653\n",
            "[ 465 ] Training Loss 2.603---Training Accuracy: 9.650\n",
            "[ 470 ] Training Loss 2.602---Training Accuracy: 9.674\n",
            "[ 475 ] Training Loss 2.601---Training Accuracy: 9.657\n",
            "[ 480 ] Training Loss 2.599---Training Accuracy: 9.687\n",
            "[ 485 ] Training Loss 2.598---Training Accuracy: 9.651\n",
            "[ 490 ] Training Loss 2.597---Training Accuracy: 9.642\n",
            "[ 495 ] Training Loss 2.596---Training Accuracy: 9.596\n",
            "[ 500 ] Training Loss 2.594---Training Accuracy: 9.575\n",
            "[ 505 ] Training Loss 2.593---Training Accuracy: 9.560\n",
            "[ 510 ] Training Loss 2.592---Training Accuracy: 9.565\n",
            "[ 515 ] Training Loss 2.591---Training Accuracy: 9.532\n",
            "[ 520 ] Training Loss 2.590---Training Accuracy: 9.531\n",
            "[ 525 ] Training Loss 2.589---Training Accuracy: 9.524\n",
            "[ 530 ] Training Loss 2.588---Training Accuracy: 9.510\n",
            "[ 535 ] Training Loss 2.587---Training Accuracy: 9.532\n",
            "[ 540 ] Training Loss 2.586---Training Accuracy: 9.537\n",
            "[ 545 ] Training Loss 2.585---Training Accuracy: 9.552\n",
            "[ 550 ] Training Loss 2.584---Training Accuracy: 9.545\n",
            "[ 555 ] Training Loss 2.583---Training Accuracy: 9.572\n",
            "[ 560 ] Training Loss 2.582---Training Accuracy: 9.559\n",
            "[ 565 ] Training Loss 2.581---Training Accuracy: 9.585\n",
            "[ 570 ] Training Loss 2.580---Training Accuracy: 9.577\n",
            "[ 575 ] Training Loss 2.579---Training Accuracy: 9.587\n",
            "[ 580 ] Training Loss 2.578---Training Accuracy: 9.590\n",
            "[ 585 ] Training Loss 2.577---Training Accuracy: 9.583\n",
            "[ 590 ] Training Loss 2.576---Training Accuracy: 9.565\n",
            "[ 595 ] Training Loss 2.575---Training Accuracy: 9.574\n",
            "[ 600 ] Training Loss 2.574---Training Accuracy: 9.609\n",
            "[ 605 ] Training Loss 2.574---Training Accuracy: 9.628\n",
            "[ 610 ] Training Loss 2.573---Training Accuracy: 9.615\n",
            "[ 615 ] Training Loss 2.572---Training Accuracy: 9.593\n",
            "[ 620 ] Training Loss 2.571---Training Accuracy: 9.596\n",
            "[ 625 ] Training Loss 2.570---Training Accuracy: 9.605\n",
            "[ 630 ] Training Loss 2.570---Training Accuracy: 9.603\n",
            "[ 635 ] Training Loss 2.569---Training Accuracy: 9.606\n",
            "[ 640 ] Training Loss 2.568---Training Accuracy: 9.619\n",
            "[ 645 ] Training Loss 2.568---Training Accuracy: 9.607\n",
            "[ 650 ] Training Loss 2.567---Training Accuracy: 9.625\n",
            "[ 655 ] Training Loss 2.566---Training Accuracy: 9.618\n",
            "[ 660 ] Training Loss 2.566---Training Accuracy: 9.607\n",
            "[ 665 ] Training Loss 2.565---Training Accuracy: 9.610\n",
            "[ 670 ] Training Loss 2.564---Training Accuracy: 9.640\n",
            "[ 675 ] Training Loss 2.564---Training Accuracy: 9.643\n",
            "[ 680 ] Training Loss 2.563---Training Accuracy: 9.664\n",
            "[ 685 ] Training Loss 2.563---Training Accuracy: 9.667\n",
            "[ 690 ] Training Loss 2.562---Training Accuracy: 9.651\n",
            "[ 695 ] Training Loss 2.561---Training Accuracy: 9.662\n",
            "[ 700 ] Training Loss 2.561---Training Accuracy: 9.665\n",
            "[ 705 ] Training Loss 2.560---Training Accuracy: 9.685\n",
            "[ 710 ] Training Loss 2.560---Training Accuracy: 9.687\n",
            "[ 715 ] Training Loss 2.559---Training Accuracy: 9.689\n",
            "[ 720 ] Training Loss 2.559---Training Accuracy: 9.674\n",
            "[ 725 ] Training Loss 2.558---Training Accuracy: 9.659\n",
            "[ 730 ] Training Loss 2.557---Training Accuracy: 9.687\n",
            "[ 735 ] Training Loss 2.557---Training Accuracy: 9.685\n",
            "[ 740 ] Training Loss 2.556---Training Accuracy: 9.683\n",
            "[ 745 ] Training Loss 2.556---Training Accuracy: 9.702\n",
            "[ 750 ] Training Loss 2.555---Training Accuracy: 9.716\n",
            "[ 755 ] Training Loss 2.555---Training Accuracy: 9.714\n",
            "[ 760 ] Training Loss 2.554---Training Accuracy: 9.757\n",
            "[ 765 ] Training Loss 2.553---Training Accuracy: 9.758\n",
            "[ 770 ] Training Loss 2.553---Training Accuracy: 9.760\n",
            "[ 775 ] Training Loss 2.552---Training Accuracy: 9.745\n",
            "[ 780 ] Training Loss 2.552---Training Accuracy: 9.751\n",
            "[ 785 ] Training Loss 2.551---Training Accuracy: 9.773\n",
            "[ 790 ] Training Loss 2.551---Training Accuracy: 9.770\n",
            "[ 795 ] Training Loss 2.550---Training Accuracy: 9.768\n",
            "[ 800 ] Training Loss 2.550---Training Accuracy: 9.773\n",
            "[ 805 ] Training Loss 2.549---Training Accuracy: 9.759\n",
            "[ 810 ] Training Loss 2.549---Training Accuracy: 9.733\n",
            "[ 815 ] Training Loss 2.548---Training Accuracy: 9.720\n",
            "[ 820 ] Training Loss 2.548---Training Accuracy: 9.737\n",
            "[ 825 ] Training Loss 2.547---Training Accuracy: 9.738\n",
            "[ 830 ] Training Loss 2.547---Training Accuracy: 9.721\n",
            "[ 835 ] Training Loss 2.547---Training Accuracy: 9.741\n",
            "[ 840 ] Training Loss 2.546---Training Accuracy: 9.739\n",
            "[ 845 ] Training Loss 2.546---Training Accuracy: 9.741\n",
            "[ 850 ] Training Loss 2.545---Training Accuracy: 9.753\n",
            "[ 855 ] Training Loss 2.545---Training Accuracy: 9.755\n",
            "[ 860 ] Training Loss 2.545---Training Accuracy: 9.760\n",
            "[ 865 ] Training Loss 2.544---Training Accuracy: 9.729\n",
            "[ 870 ] Training Loss 2.544---Training Accuracy: 9.737\n",
            "[ 875 ] Training Loss 2.543---Training Accuracy: 9.725\n",
            "[ 880 ] Training Loss 2.543---Training Accuracy: 9.708\n",
            "[ 885 ] Training Loss 2.543---Training Accuracy: 9.682\n",
            "[ 890 ] Training Loss 2.542---Training Accuracy: 9.684\n",
            "[ 895 ] Training Loss 2.542---Training Accuracy: 9.661\n",
            "[ 900 ] Training Loss 2.541---Training Accuracy: 9.687\n",
            "[ 905 ] Training Loss 2.541---Training Accuracy: 9.696\n",
            "[ 910 ] Training Loss 2.541---Training Accuracy: 9.680\n",
            "[ 915 ] Training Loss 2.540---Training Accuracy: 9.706\n",
            "[ 920 ] Training Loss 2.540---Training Accuracy: 9.697\n",
            "[ 925 ] Training Loss 2.539---Training Accuracy: 9.709\n",
            "[ 930 ] Training Loss 2.539---Training Accuracy: 9.727\n",
            "[ 935 ] Training Loss 2.539---Training Accuracy: 9.746\n",
            "[ 940 ] Training Loss 2.538---Training Accuracy: 9.740\n",
            "[ 945 ] Training Loss 2.538---Training Accuracy: 9.728\n",
            "[ 950 ] Training Loss 2.538---Training Accuracy: 9.723\n",
            "[ 955 ] Training Loss 2.537---Training Accuracy: 9.715\n",
            "[ 960 ] Training Loss 2.537---Training Accuracy: 9.723\n",
            "[ 965 ] Training Loss 2.537---Training Accuracy: 9.728\n",
            "[ 970 ] Training Loss 2.536---Training Accuracy: 9.726\n",
            "[ 975 ] Training Loss 2.536---Training Accuracy: 9.727\n",
            "[ 980 ] Training Loss 2.536---Training Accuracy: 9.719\n",
            "[ 985 ] Training Loss 2.536---Training Accuracy: 9.717\n",
            "[ 990 ] Training Loss 2.535---Training Accuracy: 9.757\n",
            "[ 995 ] Training Loss 2.535---Training Accuracy: 9.752\n",
            "[ 1000 ] Training Loss 2.535---Training Accuracy: 9.740\n",
            "[ 1005 ] Training Loss 2.535---Training Accuracy: 9.748\n",
            "[ 1010 ] Training Loss 2.534---Training Accuracy: 9.734\n",
            "[ 1015 ] Training Loss 2.534---Training Accuracy: 9.738\n",
            "[ 1020 ] Training Loss 2.534---Training Accuracy: 9.730\n",
            "[ 1025 ] Training Loss 2.533---Training Accuracy: 9.725\n",
            "[ 1030 ] Training Loss 2.533---Training Accuracy: 9.711\n",
            "[ 1035 ] Training Loss 2.533---Training Accuracy: 9.731\n",
            "[ 1040 ] Training Loss 2.533---Training Accuracy: 9.750\n",
            "[ 1045 ] Training Loss 2.532---Training Accuracy: 9.763\n",
            "[ 1050 ] Training Loss 2.532---Training Accuracy: 9.750\n",
            "[ 1055 ] Training Loss 2.532---Training Accuracy: 9.748\n",
            "[ 1060 ] Training Loss 2.531---Training Accuracy: 9.740\n",
            "[ 1065 ] Training Loss 2.531---Training Accuracy: 9.724\n",
            "[ 1070 ] Training Loss 2.531---Training Accuracy: 9.716\n",
            "[ 1075 ] Training Loss 2.531---Training Accuracy: 9.729\n",
            "[ 1080 ] Training Loss 2.531---Training Accuracy: 9.713\n",
            "[ 1085 ] Training Loss 2.530---Training Accuracy: 9.706\n",
            "[ 1090 ] Training Loss 2.530---Training Accuracy: 9.710\n",
            "[ 1095 ] Training Loss 2.530---Training Accuracy: 9.717\n",
            "[ 1100 ] Training Loss 2.529---Training Accuracy: 9.704\n",
            "[ 1105 ] Training Loss 2.529---Training Accuracy: 9.703\n",
            "[ 1110 ] Training Loss 2.529---Training Accuracy: 9.718\n",
            "[ 1115 ] Training Loss 2.529---Training Accuracy: 9.708\n",
            "[ 1120 ] Training Loss 2.529---Training Accuracy: 9.693\n",
            "[ 1125 ] Training Loss 2.528---Training Accuracy: 9.694\n",
            "[ 1130 ] Training Loss 2.528---Training Accuracy: 9.690\n",
            "[ 1135 ] Training Loss 2.528---Training Accuracy: 9.678\n",
            "[ 1140 ] Training Loss 2.528---Training Accuracy: 9.660\n",
            "[ 1145 ] Training Loss 2.527---Training Accuracy: 9.664\n",
            "[ 1150 ] Training Loss 2.527---Training Accuracy: 9.666\n",
            "[ 1155 ] Training Loss 2.527---Training Accuracy: 9.662\n",
            "[ 1160 ] Training Loss 2.527---Training Accuracy: 9.652\n",
            "[ 1165 ] Training Loss 2.526---Training Accuracy: 9.646\n",
            "[ 1170 ] Training Loss 2.526---Training Accuracy: 9.634\n",
            "[ 1175 ] Training Loss 2.526---Training Accuracy: 9.635\n",
            "[ 1180 ] Training Loss 2.526---Training Accuracy: 9.632\n",
            "[ 1185 ] Training Loss 2.526---Training Accuracy: 9.644\n",
            "[ 1190 ] Training Loss 2.525---Training Accuracy: 9.658\n",
            "[ 1195 ] Training Loss 2.525---Training Accuracy: 9.662\n",
            "[ 1200 ] Training Loss 2.525---Training Accuracy: 9.672\n",
            "[ 1205 ] Training Loss 2.525---Training Accuracy: 9.683\n",
            "[ 1210 ] Training Loss 2.525---Training Accuracy: 9.692\n",
            "[ 1215 ] Training Loss 2.524---Training Accuracy: 9.691\n",
            "[ 1220 ] Training Loss 2.524---Training Accuracy: 9.690\n",
            "[ 1225 ] Training Loss 2.524---Training Accuracy: 9.689\n",
            "[ 1230 ] Training Loss 2.524---Training Accuracy: 9.700\n",
            "[ 1235 ] Training Loss 2.524---Training Accuracy: 9.701\n",
            "[ 1240 ] Training Loss 2.524---Training Accuracy: 9.702\n",
            "[ 1245 ] Training Loss 2.523---Training Accuracy: 9.696\n",
            "[ 1250 ] Training Loss 2.523---Training Accuracy: 9.705\n",
            "[ 1255 ] Training Loss 2.523---Training Accuracy: 9.713\n",
            "[ 1260 ] Training Loss 2.523---Training Accuracy: 9.705\n",
            "[ 1265 ] Training Loss 2.523---Training Accuracy: 9.706\n",
            "[ 1270 ] Training Loss 2.523---Training Accuracy: 9.695\n",
            "[ 1275 ] Training Loss 2.522---Training Accuracy: 9.679\n",
            "[ 1280 ] Training Loss 2.522---Training Accuracy: 9.680\n",
            "[ 1285 ] Training Loss 2.522---Training Accuracy: 9.671\n",
            "[ 1290 ] Training Loss 2.522---Training Accuracy: 9.665\n",
            "[ 1295 ] Training Loss 2.522---Training Accuracy: 9.679\n",
            "[ 1300 ] Training Loss 2.521---Training Accuracy: 9.675\n",
            "[ 1305 ] Training Loss 2.521---Training Accuracy: 9.679\n",
            "[ 1310 ] Training Loss 2.521---Training Accuracy: 9.666\n",
            "[ 1315 ] Training Loss 2.521---Training Accuracy: 9.669\n",
            "[ 1320 ] Training Loss 2.521---Training Accuracy: 9.683\n",
            "[ 1325 ] Training Loss 2.521---Training Accuracy: 9.696\n",
            "[ 1330 ] Training Loss 2.521---Training Accuracy: 9.701\n",
            "[ 1335 ] Training Loss 2.520---Training Accuracy: 9.702\n",
            "[ 1340 ] Training Loss 2.520---Training Accuracy: 9.718\n",
            "[ 1345 ] Training Loss 2.520---Training Accuracy: 9.709\n",
            "[ 1350 ] Training Loss 2.520---Training Accuracy: 9.710\n",
            "[ 1355 ] Training Loss 2.520---Training Accuracy: 9.682\n",
            "[ 1360 ] Training Loss 2.520---Training Accuracy: 9.694\n",
            "[ 1365 ] Training Loss 2.520---Training Accuracy: 9.704\n",
            "[ 1370 ] Training Loss 2.520---Training Accuracy: 9.696\n",
            "[ 1375 ] Training Loss 2.519---Training Accuracy: 9.697\n",
            "[ 1380 ] Training Loss 2.519---Training Accuracy: 9.687\n",
            "[ 1385 ] Training Loss 2.519---Training Accuracy: 9.697\n",
            "[ 1390 ] Training Loss 2.519---Training Accuracy: 9.710\n",
            "[ 1395 ] Training Loss 2.519---Training Accuracy: 9.715\n",
            "[ 1400 ] Training Loss 2.519---Training Accuracy: 9.710\n",
            "[ 1405 ] Training Loss 2.518---Training Accuracy: 9.704\n",
            "[ 1410 ] Training Loss 2.518---Training Accuracy: 9.694\n",
            "[ 1415 ] Training Loss 2.518---Training Accuracy: 9.682\n",
            "[ 1420 ] Training Loss 2.518---Training Accuracy: 9.689\n",
            "[ 1425 ] Training Loss 2.518---Training Accuracy: 9.693\n",
            "[ 1430 ] Training Loss 2.518---Training Accuracy: 9.713\n",
            "[ 1435 ] Training Loss 2.517---Training Accuracy: 9.710\n",
            "[ 1440 ] Training Loss 2.517---Training Accuracy: 9.707\n",
            "[ 1445 ] Training Loss 2.517---Training Accuracy: 9.701\n",
            "[ 1450 ] Training Loss 2.517---Training Accuracy: 9.709\n",
            "[ 1455 ] Training Loss 2.517---Training Accuracy: 9.714\n",
            "[ 1460 ] Training Loss 2.517---Training Accuracy: 9.711\n",
            "[ 1465 ] Training Loss 2.517---Training Accuracy: 9.720\n",
            "[ 1470 ] Training Loss 2.516---Training Accuracy: 9.717\n",
            "[ 1475 ] Training Loss 2.516---Training Accuracy: 9.712\n",
            "[ 1480 ] Training Loss 2.516---Training Accuracy: 9.710\n",
            "[ 1485 ] Training Loss 2.516---Training Accuracy: 9.711\n",
            "[ 1490 ] Training Loss 2.516---Training Accuracy: 9.710\n",
            "[ 1495 ] Training Loss 2.516---Training Accuracy: 9.718\n",
            "[ 1500 ] Training Loss 2.516---Training Accuracy: 9.733\n",
            "[ 1505 ] Training Loss 2.516---Training Accuracy: 9.724\n",
            "[ 1510 ] Training Loss 2.516---Training Accuracy: 9.720\n",
            "[ 1515 ] Training Loss 2.516---Training Accuracy: 9.707\n",
            "[ 1520 ] Training Loss 2.515---Training Accuracy: 9.712\n",
            "[ 1525 ] Training Loss 2.515---Training Accuracy: 9.709\n",
            "[ 1530 ] Training Loss 2.515---Training Accuracy: 9.710\n",
            "[ 1535 ] Training Loss 2.515---Training Accuracy: 9.703\n",
            "[ 1540 ] Training Loss 2.515---Training Accuracy: 9.712\n",
            "[ 1545 ] Training Loss 2.515---Training Accuracy: 9.711\n",
            "[ 1550 ] Training Loss 2.515---Training Accuracy: 9.705\n",
            "[ 1555 ] Training Loss 2.515---Training Accuracy: 9.708\n",
            "[ 1560 ] Training Loss 2.515---Training Accuracy: 9.701\n",
            "[ 1565 ] Training Loss 2.514---Training Accuracy: 9.702\n",
            "[ 1570 ] Training Loss 2.514---Training Accuracy: 9.699\n",
            "[ 1575 ] Training Loss 2.514---Training Accuracy: 9.690\n",
            "[ 1580 ] Training Loss 2.514---Training Accuracy: 9.689\n",
            "[ 1585 ] Training Loss 2.514---Training Accuracy: 9.682\n",
            "[ 1590 ] Training Loss 2.514---Training Accuracy: 9.683\n",
            "[ 1595 ] Training Loss 2.514---Training Accuracy: 9.684\n",
            "[ 1600 ] Training Loss 2.514---Training Accuracy: 9.691\n",
            "[ 1605 ] Training Loss 2.513---Training Accuracy: 9.696\n",
            "[ 1610 ] Training Loss 2.513---Training Accuracy: 9.705\n",
            "[ 1615 ] Training Loss 2.513---Training Accuracy: 9.711\n",
            "[ 1620 ] Training Loss 2.513---Training Accuracy: 9.710\n",
            "[ 1625 ] Training Loss 2.513---Training Accuracy: 9.715\n",
            "[ 1630 ] Training Loss 2.513---Training Accuracy: 9.703\n",
            "[ 1635 ] Training Loss 2.513---Training Accuracy: 9.698\n",
            "[ 1640 ] Training Loss 2.513---Training Accuracy: 9.699\n",
            "[ 1645 ] Training Loss 2.513---Training Accuracy: 9.700\n",
            "[ 1650 ] Training Loss 2.512---Training Accuracy: 9.687\n",
            "[ 1655 ] Training Loss 2.512---Training Accuracy: 9.698\n",
            "[ 1660 ] Training Loss 2.512---Training Accuracy: 9.695\n",
            "[ 1665 ] Training Loss 2.512---Training Accuracy: 9.701\n",
            "[ 1670 ] Training Loss 2.512---Training Accuracy: 9.693\n",
            "[ 1675 ] Training Loss 2.512---Training Accuracy: 9.685\n",
            "[ 1680 ] Training Loss 2.512---Training Accuracy: 9.678\n",
            "[ 1685 ] Training Loss 2.512---Training Accuracy: 9.668\n",
            "[ 1690 ] Training Loss 2.512---Training Accuracy: 9.669\n",
            "[ 1695 ] Training Loss 2.512---Training Accuracy: 9.664\n",
            "[ 1700 ] Training Loss 2.511---Training Accuracy: 9.680\n",
            "[ 1705 ] Training Loss 2.511---Training Accuracy: 9.683\n",
            "[ 1710 ] Training Loss 2.511---Training Accuracy: 9.678\n",
            "[ 1715 ] Training Loss 2.511---Training Accuracy: 9.677\n",
            "[ 1720 ] Training Loss 2.511---Training Accuracy: 9.689\n",
            "[ 1725 ] Training Loss 2.511---Training Accuracy: 9.697\n",
            "[ 1730 ] Training Loss 2.511---Training Accuracy: 9.695\n",
            "[ 1735 ] Training Loss 2.510---Training Accuracy: 9.695\n",
            "[ 1740 ] Training Loss 2.510---Training Accuracy: 9.693\n",
            "[ 1745 ] Training Loss 2.510---Training Accuracy: 9.695\n",
            "[ 1750 ] Training Loss 2.510---Training Accuracy: 9.705\n",
            "[ 1755 ] Training Loss 2.510---Training Accuracy: 9.713\n",
            "[ 1760 ] Training Loss 2.510---Training Accuracy: 9.717\n",
            "[ 1765 ] Training Loss 2.510---Training Accuracy: 9.731\n",
            "[ 1770 ] Training Loss 2.510---Training Accuracy: 9.731\n",
            "[ 1775 ] Training Loss 2.510---Training Accuracy: 9.741\n",
            "[ 1780 ] Training Loss 2.510---Training Accuracy: 9.740\n",
            "[ 1785 ] Training Loss 2.509---Training Accuracy: 9.732\n",
            "[ 1790 ] Training Loss 2.509---Training Accuracy: 9.729\n",
            "[ 1795 ] Training Loss 2.509---Training Accuracy: 9.737\n",
            "[ 1800 ] Training Loss 2.509---Training Accuracy: 9.741\n",
            "[ 1805 ] Training Loss 2.509---Training Accuracy: 9.733\n",
            "[ 1810 ] Training Loss 2.509---Training Accuracy: 9.734\n",
            "[ 1815 ] Training Loss 2.509---Training Accuracy: 9.728\n",
            "[ 1820 ] Training Loss 2.509---Training Accuracy: 9.727\n",
            "[ 1825 ] Training Loss 2.509---Training Accuracy: 9.738\n",
            "[ 1830 ] Training Loss 2.509---Training Accuracy: 9.756\n",
            "[ 1835 ] Training Loss 2.508---Training Accuracy: 9.763\n",
            "[ 1840 ] Training Loss 2.508---Training Accuracy: 9.755\n",
            "[ 1845 ] Training Loss 2.508---Training Accuracy: 9.753\n",
            "[ 1850 ] Training Loss 2.508---Training Accuracy: 9.753\n",
            "[ 1855 ] Training Loss 2.508---Training Accuracy: 9.754\n",
            "[ 1860 ] Training Loss 2.508---Training Accuracy: 9.760\n",
            "[ 1865 ] Training Loss 2.508---Training Accuracy: 9.762\n",
            "[ 1870 ] Training Loss 2.508---Training Accuracy: 9.761\n",
            "[ 1875 ] Training Loss 2.508---Training Accuracy: 9.756\n",
            "[ 1880 ] Training Loss 2.508---Training Accuracy: 9.750\n",
            "[ 1885 ] Training Loss 2.508---Training Accuracy: 9.743\n",
            "[ 1890 ] Training Loss 2.508---Training Accuracy: 9.735\n",
            "[ 1895 ] Training Loss 2.508---Training Accuracy: 9.736\n",
            "[ 1900 ] Training Loss 2.508---Training Accuracy: 9.733\n",
            "[ 1905 ] Training Loss 2.507---Training Accuracy: 9.732\n",
            "[ 1910 ] Training Loss 2.507---Training Accuracy: 9.730\n",
            "[ 1915 ] Training Loss 2.507---Training Accuracy: 9.719\n",
            "[ 1920 ] Training Loss 2.507---Training Accuracy: 9.730\n",
            "[ 1925 ] Training Loss 2.507---Training Accuracy: 9.729\n",
            "[ 1930 ] Training Loss 2.507---Training Accuracy: 9.733\n",
            "[ 1935 ] Training Loss 2.507---Training Accuracy: 9.727\n",
            "[ 1940 ] Training Loss 2.507---Training Accuracy: 9.728\n",
            "[ 1945 ] Training Loss 2.507---Training Accuracy: 9.730\n",
            "[ 1950 ] Training Loss 2.507---Training Accuracy: 9.734\n",
            "[ 1955 ] Training Loss 2.507---Training Accuracy: 9.725\n",
            "[ 1960 ] Training Loss 2.507---Training Accuracy: 9.726\n",
            "[ 1965 ] Training Loss 2.507---Training Accuracy: 9.723\n",
            "[ 1970 ] Training Loss 2.506---Training Accuracy: 9.719\n",
            "[ 1975 ] Training Loss 2.506---Training Accuracy: 9.713\n",
            "[ 1980 ] Training Loss 2.506---Training Accuracy: 9.717\n",
            "[ 1985 ] Training Loss 2.506---Training Accuracy: 9.709\n",
            "[ 1990 ] Training Loss 2.506---Training Accuracy: 9.709\n",
            "[ 1995 ] Training Loss 2.506---Training Accuracy: 9.710\n",
            "[ 2000 ] Training Loss 2.506---Training Accuracy: 9.711\n",
            "[ 2005 ] Training Loss 2.506---Training Accuracy: 9.705\n",
            "[ 2010 ] Training Loss 2.506---Training Accuracy: 9.698\n",
            "[ 2015 ] Training Loss 2.506---Training Accuracy: 9.697\n",
            "[ 2020 ] Training Loss 2.506---Training Accuracy: 9.697\n",
            "[ 2025 ] Training Loss 2.506---Training Accuracy: 9.694\n",
            "[ 2030 ] Training Loss 2.506---Training Accuracy: 9.690\n",
            "[ 2035 ] Training Loss 2.506---Training Accuracy: 9.690\n",
            "[ 2040 ] Training Loss 2.506---Training Accuracy: 9.695\n",
            "[ 2045 ] Training Loss 2.505---Training Accuracy: 9.693\n",
            "[ 2050 ] Training Loss 2.505---Training Accuracy: 9.690\n",
            "[ 2055 ] Training Loss 2.505---Training Accuracy: 9.688\n",
            "[ 2060 ] Training Loss 2.505---Training Accuracy: 9.677\n",
            "[ 2065 ] Training Loss 2.505---Training Accuracy: 9.678\n",
            "[ 2070 ] Training Loss 2.505---Training Accuracy: 9.672\n",
            "[ 2075 ] Training Loss 2.505---Training Accuracy: 9.663\n",
            "[ 2080 ] Training Loss 2.505---Training Accuracy: 9.662\n",
            "[ 2085 ] Training Loss 2.505---Training Accuracy: 9.663\n",
            "[ 2090 ] Training Loss 2.505---Training Accuracy: 9.669\n",
            "[ 2095 ] Training Loss 2.505---Training Accuracy: 9.667\n",
            "[ 2100 ] Training Loss 2.505---Training Accuracy: 9.672\n",
            "[ 2105 ] Training Loss 2.505---Training Accuracy: 9.666\n",
            "[ 2110 ] Training Loss 2.505---Training Accuracy: 9.664\n",
            "[ 2115 ] Training Loss 2.505---Training Accuracy: 9.667\n",
            "[ 2120 ] Training Loss 2.504---Training Accuracy: 9.667\n",
            "[ 2125 ] Training Loss 2.504---Training Accuracy: 9.668\n",
            "[ 2130 ] Training Loss 2.504---Training Accuracy: 9.662\n",
            "[ 2135 ] Training Loss 2.504---Training Accuracy: 9.668\n",
            "[ 2140 ] Training Loss 2.504---Training Accuracy: 9.667\n",
            "[ 2145 ] Training Loss 2.504---Training Accuracy: 9.665\n",
            "[ 2150 ] Training Loss 2.504---Training Accuracy: 9.663\n",
            "[ 2155 ] Training Loss 2.504---Training Accuracy: 9.665\n",
            "[ 2160 ] Training Loss 2.504---Training Accuracy: 9.653\n",
            "[ 2165 ] Training Loss 2.504---Training Accuracy: 9.661\n",
            "[ 2170 ] Training Loss 2.504---Training Accuracy: 9.661\n",
            "[ 2175 ] Training Loss 2.504---Training Accuracy: 9.658\n",
            "[ 2180 ] Training Loss 2.504---Training Accuracy: 9.659\n",
            "[ 2185 ] Training Loss 2.504---Training Accuracy: 9.655\n",
            "[ 2190 ] Training Loss 2.504---Training Accuracy: 9.643\n",
            "[ 2195 ] Training Loss 2.504---Training Accuracy: 9.644\n",
            "[ 2200 ] Training Loss 2.503---Training Accuracy: 9.646\n",
            "[ 2205 ] Training Loss 2.503---Training Accuracy: 9.647\n",
            "[ 2210 ] Training Loss 2.503---Training Accuracy: 9.646\n",
            "[ 2215 ] Training Loss 2.503---Training Accuracy: 9.650\n",
            "[ 2220 ] Training Loss 2.503---Training Accuracy: 9.649\n",
            "[ 2225 ] Training Loss 2.503---Training Accuracy: 9.650\n",
            "[ 2230 ] Training Loss 2.503---Training Accuracy: 9.645\n",
            "[ 2235 ] Training Loss 2.503---Training Accuracy: 9.645\n",
            "[ 2240 ] Training Loss 2.503---Training Accuracy: 9.641\n",
            "[ 2245 ] Training Loss 2.503---Training Accuracy: 9.641\n",
            "[ 2250 ] Training Loss 2.503---Training Accuracy: 9.643\n",
            "[ 2255 ] Training Loss 2.503---Training Accuracy: 9.641\n",
            "[ 2260 ] Training Loss 2.503---Training Accuracy: 9.635\n",
            "[ 2265 ] Training Loss 2.503---Training Accuracy: 9.647\n",
            "[ 2270 ] Training Loss 2.503---Training Accuracy: 9.653\n",
            "[ 2275 ] Training Loss 2.502---Training Accuracy: 9.644\n",
            "[ 2280 ] Training Loss 2.502---Training Accuracy: 9.642\n",
            "[ 2285 ] Training Loss 2.502---Training Accuracy: 9.639\n",
            "[ 2290 ] Training Loss 2.502---Training Accuracy: 9.637\n",
            "[ 2295 ] Training Loss 2.502---Training Accuracy: 9.644\n",
            "[ 2300 ] Training Loss 2.502---Training Accuracy: 9.644\n",
            "[ 2305 ] Training Loss 2.502---Training Accuracy: 9.649\n",
            "[ 2310 ] Training Loss 2.502---Training Accuracy: 9.647\n",
            "[ 2315 ] Training Loss 2.502---Training Accuracy: 9.641\n",
            "[ 2320 ] Training Loss 2.502---Training Accuracy: 9.640\n",
            "[ 2325 ] Training Loss 2.502---Training Accuracy: 9.634\n",
            "[ 2330 ] Training Loss 2.502---Training Accuracy: 9.631\n",
            "[ 2335 ] Training Loss 2.502---Training Accuracy: 9.631\n",
            "[ 2340 ] Training Loss 2.502---Training Accuracy: 9.622\n",
            "[ 2345 ] Training Loss 2.502---Training Accuracy: 9.624\n",
            "[ 2350 ] Training Loss 2.502---Training Accuracy: 9.625\n",
            "[ 2355 ] Training Loss 2.501---Training Accuracy: 9.630\n",
            "[ 2360 ] Training Loss 2.501---Training Accuracy: 9.634\n",
            "[ 2365 ] Training Loss 2.501---Training Accuracy: 9.635\n",
            "[ 2370 ] Training Loss 2.501---Training Accuracy: 9.640\n",
            "[ 2375 ] Training Loss 2.501---Training Accuracy: 9.633\n",
            "[ 2380 ] Training Loss 2.501---Training Accuracy: 9.627\n",
            "[ 2385 ] Training Loss 2.501---Training Accuracy: 9.630\n",
            "[ 2390 ] Training Loss 2.501---Training Accuracy: 9.639\n",
            "[ 2395 ] Training Loss 2.501---Training Accuracy: 9.638\n",
            "[ 2400 ] Training Loss 2.501---Training Accuracy: 9.642\n",
            "[ 2405 ] Training Loss 2.501---Training Accuracy: 9.640\n",
            "[ 2410 ] Training Loss 2.501---Training Accuracy: 9.636\n",
            "[ 2415 ] Training Loss 2.501---Training Accuracy: 9.638\n",
            "[ 2420 ] Training Loss 2.501---Training Accuracy: 9.629\n",
            "[ 2425 ] Training Loss 2.501---Training Accuracy: 9.639\n",
            "[ 2430 ] Training Loss 2.501---Training Accuracy: 9.639\n",
            "[ 2435 ] Training Loss 2.501---Training Accuracy: 9.635\n",
            "[ 2440 ] Training Loss 2.501---Training Accuracy: 9.640\n",
            "[ 2445 ] Training Loss 2.501---Training Accuracy: 9.636\n",
            "[ 2450 ] Training Loss 2.500---Training Accuracy: 9.630\n",
            "[ 2455 ] Training Loss 2.500---Training Accuracy: 9.622\n",
            "[ 2460 ] Training Loss 2.500---Training Accuracy: 9.623\n",
            "[ 2465 ] Training Loss 2.500---Training Accuracy: 9.626\n",
            "[ 2470 ] Training Loss 2.500---Training Accuracy: 9.625\n",
            "[ 2475 ] Training Loss 2.500---Training Accuracy: 9.627\n",
            "[ 2480 ] Training Loss 2.500---Training Accuracy: 9.626\n",
            "[ 2485 ] Training Loss 2.500---Training Accuracy: 9.631\n",
            "[ 2490 ] Training Loss 2.500---Training Accuracy: 9.628\n",
            "[ 2495 ] Training Loss 2.500---Training Accuracy: 9.629\n",
            "[ 2500 ] Training Loss 2.500---Training Accuracy: 9.632\n",
            "[ 2505 ] Training Loss 2.500---Training Accuracy: 9.633\n",
            "[ 2510 ] Training Loss 2.500---Training Accuracy: 9.626\n",
            "[ 2515 ] Training Loss 2.500---Training Accuracy: 9.616\n",
            "[ 2520 ] Training Loss 2.500---Training Accuracy: 9.606\n",
            "[ 2525 ] Training Loss 2.500---Training Accuracy: 9.614\n",
            "[ 2530 ] Training Loss 2.500---Training Accuracy: 9.613\n",
            "[ 2535 ] Training Loss 2.500---Training Accuracy: 9.618\n",
            "[ 2540 ] Training Loss 2.500---Training Accuracy: 9.621\n",
            "[ 2545 ] Training Loss 2.500---Training Accuracy: 9.625\n",
            "[ 2550 ] Training Loss 2.500---Training Accuracy: 9.630\n",
            "[ 2555 ] Training Loss 2.500---Training Accuracy: 9.633\n",
            "[ 2560 ] Training Loss 2.500---Training Accuracy: 9.631\n",
            "[ 2565 ] Training Loss 2.499---Training Accuracy: 9.627\n",
            "[ 2570 ] Training Loss 2.499---Training Accuracy: 9.631\n",
            "[ 2575 ] Training Loss 2.499---Training Accuracy: 9.630\n",
            "[ 2580 ] Training Loss 2.499---Training Accuracy: 9.627\n",
            "[ 2585 ] Training Loss 2.499---Training Accuracy: 9.631\n",
            "[ 2590 ] Training Loss 2.499---Training Accuracy: 9.628\n",
            "[ 2595 ] Training Loss 2.499---Training Accuracy: 9.627\n",
            "[ 2600 ] Training Loss 2.499---Training Accuracy: 9.632\n",
            "[ 2605 ] Training Loss 2.499---Training Accuracy: 9.635\n",
            "[ 2610 ] Training Loss 2.499---Training Accuracy: 9.631\n",
            "[ 2615 ] Training Loss 2.499---Training Accuracy: 9.629\n",
            "[ 2620 ] Training Loss 2.499---Training Accuracy: 9.628\n",
            "[ 2625 ] Training Loss 2.499---Training Accuracy: 9.627\n",
            "[ 2630 ] Training Loss 2.499---Training Accuracy: 9.626\n",
            "[ 2635 ] Training Loss 2.499---Training Accuracy: 9.630\n",
            "[ 2640 ] Training Loss 2.499---Training Accuracy: 9.640\n",
            "[ 2645 ] Training Loss 2.499---Training Accuracy: 9.638\n",
            "[ 2650 ] Training Loss 2.499---Training Accuracy: 9.637\n",
            "[ 2655 ] Training Loss 2.499---Training Accuracy: 9.639\n",
            "[ 2660 ] Training Loss 2.499---Training Accuracy: 9.638\n",
            "[ 2665 ] Training Loss 2.499---Training Accuracy: 9.632\n",
            "[ 2670 ] Training Loss 2.499---Training Accuracy: 9.645\n",
            "[ 2675 ] Training Loss 2.498---Training Accuracy: 9.644\n",
            "[ 2680 ] Training Loss 2.498---Training Accuracy: 9.643\n",
            "[ 2685 ] Training Loss 2.498---Training Accuracy: 9.645\n",
            "[ 2690 ] Training Loss 2.498---Training Accuracy: 9.648\n",
            "[ 2695 ] Training Loss 2.498---Training Accuracy: 9.662\n",
            "[ 2700 ] Training Loss 2.498---Training Accuracy: 9.662\n",
            "[ 2705 ] Training Loss 2.498---Training Accuracy: 9.659\n",
            "[ 2710 ] Training Loss 2.498---Training Accuracy: 9.659\n",
            "[ 2715 ] Training Loss 2.498---Training Accuracy: 9.660\n",
            "[ 2720 ] Training Loss 2.498---Training Accuracy: 9.656\n",
            "[ 2725 ] Training Loss 2.498---Training Accuracy: 9.658\n",
            "[ 2730 ] Training Loss 2.498---Training Accuracy: 9.658\n",
            "[ 2735 ] Training Loss 2.498---Training Accuracy: 9.653\n",
            "[ 2740 ] Training Loss 2.498---Training Accuracy: 9.653\n",
            "[ 2745 ] Training Loss 2.498---Training Accuracy: 9.649\n",
            "[ 2750 ] Training Loss 2.498---Training Accuracy: 9.653\n",
            "[ 2755 ] Training Loss 2.498---Training Accuracy: 9.645\n",
            "[ 2760 ] Training Loss 2.498---Training Accuracy: 9.636\n",
            "[ 2765 ] Training Loss 2.498---Training Accuracy: 9.637\n",
            "[ 2770 ] Training Loss 2.498---Training Accuracy: 9.638\n",
            "[ 2775 ] Training Loss 2.498---Training Accuracy: 9.634\n",
            "[ 2780 ] Training Loss 2.498---Training Accuracy: 9.627\n",
            "[ 2785 ] Training Loss 2.498---Training Accuracy: 9.632\n",
            "[ 2790 ] Training Loss 2.498---Training Accuracy: 9.629\n",
            "[ 2795 ] Training Loss 2.498---Training Accuracy: 9.623\n",
            "[ 2800 ] Training Loss 2.498---Training Accuracy: 9.623\n",
            "[ 2805 ] Training Loss 2.498---Training Accuracy: 9.624\n",
            "[ 2810 ] Training Loss 2.497---Training Accuracy: 9.628\n",
            "[ 2815 ] Training Loss 2.497---Training Accuracy: 9.621\n",
            "[ 2820 ] Training Loss 2.497---Training Accuracy: 9.625\n",
            "[ 2825 ] Training Loss 2.497---Training Accuracy: 9.623\n",
            "[ 2830 ] Training Loss 2.497---Training Accuracy: 9.627\n",
            "[ 2835 ] Training Loss 2.497---Training Accuracy: 9.631\n",
            "[ 2840 ] Training Loss 2.497---Training Accuracy: 9.631\n",
            "[ 2845 ] Training Loss 2.497---Training Accuracy: 9.632\n",
            "[ 2850 ] Training Loss 2.497---Training Accuracy: 9.631\n",
            "[ 2855 ] Training Loss 2.497---Training Accuracy: 9.637\n",
            "[ 2860 ] Training Loss 2.497---Training Accuracy: 9.632\n",
            "[ 2865 ] Training Loss 2.497---Training Accuracy: 9.628\n",
            "[ 2870 ] Training Loss 2.497---Training Accuracy: 9.622\n",
            "[ 2875 ] Training Loss 2.497---Training Accuracy: 9.618\n",
            "[ 2880 ] Training Loss 2.497---Training Accuracy: 9.608\n",
            "[ 2885 ] Training Loss 2.497---Training Accuracy: 9.615\n",
            "[ 2890 ] Training Loss 2.497---Training Accuracy: 9.615\n",
            "[ 2895 ] Training Loss 2.497---Training Accuracy: 9.610\n",
            "[ 2900 ] Training Loss 2.497---Training Accuracy: 9.609\n",
            "[ 2905 ] Training Loss 2.497---Training Accuracy: 9.611\n",
            "[ 2910 ] Training Loss 2.497---Training Accuracy: 9.608\n",
            "[ 2915 ] Training Loss 2.497---Training Accuracy: 9.610\n",
            "[ 2920 ] Training Loss 2.497---Training Accuracy: 9.611\n",
            "[ 2925 ] Training Loss 2.497---Training Accuracy: 9.605\n",
            "[ 2930 ] Training Loss 2.497---Training Accuracy: 9.604\n",
            "[ 2935 ] Training Loss 2.497---Training Accuracy: 9.600\n",
            "[ 2940 ] Training Loss 2.497---Training Accuracy: 9.597\n",
            "[ 2945 ] Training Loss 2.497---Training Accuracy: 9.596\n",
            "[ 2950 ] Training Loss 2.496---Training Accuracy: 9.594\n",
            "[ 2955 ] Training Loss 2.496---Training Accuracy: 9.595\n",
            "[ 2960 ] Training Loss 2.496---Training Accuracy: 9.595\n",
            "[ 2965 ] Training Loss 2.496---Training Accuracy: 9.598\n",
            "[ 2970 ] Training Loss 2.496---Training Accuracy: 9.602\n",
            "[ 2975 ] Training Loss 2.496---Training Accuracy: 9.599\n",
            "[ 2980 ] Training Loss 2.496---Training Accuracy: 9.594\n",
            "[ 2985 ] Training Loss 2.496---Training Accuracy: 9.596\n",
            "[ 2990 ] Training Loss 2.496---Training Accuracy: 9.603\n",
            "[ 2995 ] Training Loss 2.496---Training Accuracy: 9.604\n",
            "[ 3000 ] Training Loss 2.496---Training Accuracy: 9.602\n",
            "[ 3005 ] Training Loss 2.496---Training Accuracy: 9.601\n",
            "[ 3010 ] Training Loss 2.496---Training Accuracy: 9.602\n",
            "[ 3015 ] Training Loss 2.496---Training Accuracy: 9.595\n",
            "[ 3020 ] Training Loss 2.496---Training Accuracy: 9.595\n",
            "[ 3025 ] Training Loss 2.496---Training Accuracy: 9.599\n",
            "[ 3030 ] Training Loss 2.496---Training Accuracy: 9.600\n",
            "[ 3035 ] Training Loss 2.496---Training Accuracy: 9.597\n",
            "[ 3040 ] Training Loss 2.496---Training Accuracy: 9.601\n",
            "[ 3045 ] Training Loss 2.496---Training Accuracy: 9.608\n",
            "[ 3050 ] Training Loss 2.496---Training Accuracy: 9.603\n",
            "[ 3055 ] Training Loss 2.496---Training Accuracy: 9.612\n",
            "[ 3060 ] Training Loss 2.496---Training Accuracy: 9.607\n",
            "[ 3065 ] Training Loss 2.496---Training Accuracy: 9.604\n",
            "[ 3070 ] Training Loss 2.496---Training Accuracy: 9.606\n",
            "[ 3075 ] Training Loss 2.496---Training Accuracy: 9.606\n",
            "[ 3080 ] Training Loss 2.495---Training Accuracy: 9.602\n",
            "[ 3085 ] Training Loss 2.495---Training Accuracy: 9.598\n",
            "[ 3090 ] Training Loss 2.495---Training Accuracy: 9.594\n",
            "[ 3095 ] Training Loss 2.495---Training Accuracy: 9.594\n",
            "[ 3100 ] Training Loss 2.495---Training Accuracy: 9.591\n",
            "[ 3105 ] Training Loss 2.495---Training Accuracy: 9.587\n",
            "[ 3110 ] Training Loss 2.495---Training Accuracy: 9.591\n",
            "[ 3115 ] Training Loss 2.495---Training Accuracy: 9.590\n",
            "[ 3120 ] Training Loss 2.495---Training Accuracy: 9.597\n",
            "[ 3125 ] Training Loss 2.495---Training Accuracy: 9.595\n",
            "[ 3130 ] Training Loss 2.495---Training Accuracy: 9.584\n",
            "[ 3135 ] Training Loss 2.495---Training Accuracy: 9.583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e50760a65232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-e50760a65232>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mbig_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mn_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalcuate_accu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}