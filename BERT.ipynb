{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1uedKqXll7IbJTxxtdX82ESNvP1Zy9vWa",
      "authorship_tag": "ABX9TyNBJIVCzrkjkZWwc4c0W0Fn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atherfawaz/BERT-Supervised/blob/master/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca7jIEhZNU_Q",
        "colab_type": "text"
      },
      "source": [
        "# Supervised Learning with BERT\n",
        "\n",
        "***Question*: Supervised Learning with BERT\n",
        "The Astrological department believes that a person's astrological sign can be guessed from their behavior. An organization is collecting blog-posts of different people from various sources. You have been tasked to build a Deep Learning model that can use these posts data of individuals to predict which star group out of 12 does an individual belong to. You also need to tell the gender of that person.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PIs-jEzMCT4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2652ac68-bfd7-42f3-b61c-d84936632afe"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "%cd /content/drive/My Drive/Ebryx/blogs_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Ebryx/blogs_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GW1LBluMU3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xvf  'blogs_train.tar.xz'\n",
        "%cd /content/drive/My Drive/Ebryx/blogs_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ArTrha5U1RA",
        "colab_type": "text"
      },
      "source": [
        "# Dataset\n",
        "The file name would contain the gender, age, occupation, and astrological sign of the blooger. For example, 4115891.male.24.Student.Leo.xml is one file. A single file will contain a set of blogs separated by date. To illustrate, this is what a sample file looks like:\n",
        "\n",
        "```\n",
        "<Blog>\n",
        "  <date>31,May,2004</date>\n",
        "    <post>\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "      Well, everyone got up and going this morning.  It's still raining, but that's okay with me.  Sort of suits my mood.  I could easily have stayed home in bed with my book and the cats.  This has been a lot of rain though!..\n",
        "    </post>\n",
        "</Blog\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN1hRWkRd6vI",
        "colab_type": "text"
      },
      "source": [
        "# Parsing the dataset\n",
        "Parsing the dataset from separate files into a Pandas Dataframe for displaying and easy access. Some XML files contain encoding issues and the problematic contents of those files have been replaced by random number. While this could impact the accuracy of the model later, the effect would not be that big."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYh3rv4SnJ1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import codecs\n",
        "from bs4 import BeautifulSoup\n",
        "from progressbar import ProgressBar\n",
        "pbar = ProgressBar()\n",
        "\n",
        "print('PARSING FILES....')\n",
        "\n",
        "FILES = os.listdir()\n",
        "#print('File count: ', len(FILES))\n",
        "#print(FILES)\n",
        "#FILES = ['4115891.male.24.Student.Leo.xml', '4115958.male.16.Communications-Media.Libra.xml', '4116071.male.26.Arts.Sagittarius.xml', '4116243.female.24.Manufacturing.Sagittarius.xml']\n",
        "\n",
        "posts_arr = []\n",
        "sign_arr = []\n",
        "gender_arr = []\n",
        "age_arr = []\n",
        "occupation_arr = []\n",
        "\n",
        "for to_fetch in pbar(FILES):\n",
        "    #print('Parsing file:', to_fetch)\n",
        "    gender = to_fetch.split('.')[-5]\n",
        "    age = to_fetch.split('.')[-4]\n",
        "    occupation = to_fetch.split('.')[-3]\n",
        "    sign = to_fetch.split('.')[-2]\n",
        "    with codecs.open(to_fetch, 'r', encoding='utf-8', errors='ignore') as fp:\n",
        "      soup = BeautifulSoup(fp, 'lxml',\n",
        "                           from_encoding='utf8')\n",
        "      posts = soup.find_all('post')\n",
        "      #print(soup.prettify())\n",
        "      for post in posts:\n",
        "        clean_str = post.text\n",
        "        clean_str = clean_str.replace('\\r', '')\n",
        "        clean_str = clean_str.replace('\\n', '')\n",
        "        posts_arr.append(clean_str)\n",
        "        sign_arr.append(sign)\n",
        "        age_arr.append(age)\n",
        "        occupation_arr.append(occupation)\n",
        "        gender_arr.append(gender)\n",
        "\n",
        "df = pd.DataFrame({'Gender': gender_arr, 'Age': age_arr, 'Occupation': occupation_arr, 'Post': posts_arr, 'Sign': sign_arr})\n",
        "\n",
        "#df.head(50)\n",
        "df.to_csv('/content/drive/My Drive/Ebryx/dataset.csv', encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjGpg5ZfnmcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knjTjaAtcx-H",
        "colab_type": "text"
      },
      "source": [
        "# Loading DistilledBERT for fine tuning\n",
        "\n",
        "Taking help from the model implementation from huggingface and [this repository](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb#scrollTo=JrBr2YesGdO_).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKyIOfh5jmCz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "0295677c-cbc6-42a4-8f58-44f21f4797c2"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32pVsuaBj12B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "66d454b4-a63a-43e3-973b-321259f3d503"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug 17 11:23:57 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0    29W /  70W |   3755MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7amTtuAjhZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "acbf4702-00c2-401d-bc65-40ee2b316e0d"
      },
      "source": [
        "# Importing the libraries needed\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 0.01\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
        "\n",
        "# Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBPNbR2tl8sZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "9818a68d-3bdc-49d6-db0f-4f032d3948fc"
      },
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "df = pd.read_csv('/content/drive/My Drive/Ebryx/dataset.csv')\n",
        "df = df[['Post', 'Sign']]\n",
        "\n",
        "encode_dict = {}\n",
        "\n",
        "def encode_cat(x):\n",
        "    if x not in encode_dict.keys():\n",
        "        encode_dict[x]=len(encode_dict)\n",
        "    return encode_dict[x]\n",
        "\n",
        "df['Sign'] = df['Sign'].apply(lambda x: encode_cat(x))\n",
        "\n",
        "df.head(100)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Post</th>\n",
              "      <th>Sign</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I just watched Beauty and the beast...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This picture shows a Vietnamese ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So I just used the term “Bad Ass...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This is a dumb little story I whipp...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Listen, you fuckers, you screwhe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>\\t         There is a reason that I haven't be...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>\\t         After I wrote that last bit, I prom...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>\\t         Today is just not my day... I break...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>\\t         Last night Grandpa got me and asked...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>\\t         Andrew's going away and I'm sad, be...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Post  Sign\n",
              "0              I just watched Beauty and the beast...     0\n",
              "1                 This picture shows a Vietnamese ...     0\n",
              "2                 So I just used the term “Bad Ass...     0\n",
              "3              This is a dumb little story I whipp...     0\n",
              "4                 Listen, you fuckers, you screwhe...     0\n",
              "..                                                ...   ...\n",
              "95  \\t         There is a reason that I haven't be...     6\n",
              "96  \\t         After I wrote that last bit, I prom...     6\n",
              "97  \\t         Today is just not my day... I break...     6\n",
              "98  \\t         Last night Grandpa got me and asked...     6\n",
              "99  \\t         Andrew's going away and I'm sad, be...     6\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT4MFsu3kW8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5fbd34cf-5e53-4c45-ceba-f7b4c7d7c4d5"
      },
      "source": [
        "class Triage(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        post = str(self.data.Post[index])\n",
        "        post = \" \".join(post.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            post,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.data.Sign[index], dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state=200)\n",
        "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (380720, 2)\n",
            "TRAIN Dataset: (304576, 2)\n",
            "TEST Dataset: (76144, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNFZh2WQkhl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB0uZ-dgnJm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class DistillBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistillBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 12)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "model = DistillBERTClass()\n",
        "model.to(device)\n",
        "\n",
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwhi-s_AnR4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58150afa-c467-4eb0-844f-d65ff7bfdb3b"
      },
      "source": [
        "# Function to calcuate the accuracy of the model\n",
        "\n",
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct\n",
        "\n",
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for i,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        \n",
        "        loss_step = tr_loss/nb_tr_steps\n",
        "        accu_step = (n_correct*100)/nb_tr_examples \n",
        "        print(f\"[ {i} ] Training Loss {loss_step:.3f}---Training Accuracy: {accu_step:.3f}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return \n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0 ] Training Loss 2.510---Training Accuracy: 9.375\n",
            "[ 1 ] Training Loss 2.532---Training Accuracy: 7.812\n",
            "[ 2 ] Training Loss 2.946---Training Accuracy: 6.250\n",
            "[ 3 ] Training Loss 4.077---Training Accuracy: 5.469\n",
            "[ 4 ] Training Loss 3.978---Training Accuracy: 6.875\n",
            "[ 5 ] Training Loss 4.072---Training Accuracy: 6.250\n",
            "[ 6 ] Training Loss 4.140---Training Accuracy: 6.250\n",
            "[ 7 ] Training Loss 4.525---Training Accuracy: 5.859\n",
            "[ 8 ] Training Loss 4.545---Training Accuracy: 6.250\n",
            "[ 9 ] Training Loss 4.504---Training Accuracy: 6.250\n",
            "[ 10 ] Training Loss 4.390---Training Accuracy: 6.534\n",
            "[ 11 ] Training Loss 4.274---Training Accuracy: 6.510\n",
            "[ 12 ] Training Loss 4.161---Training Accuracy: 6.490\n",
            "[ 13 ] Training Loss 4.060---Training Accuracy: 6.696\n",
            "[ 14 ] Training Loss 3.977---Training Accuracy: 7.083\n",
            "[ 15 ] Training Loss 3.880---Training Accuracy: 8.008\n",
            "[ 16 ] Training Loss 3.797---Training Accuracy: 8.456\n",
            "[ 17 ] Training Loss 3.725---Training Accuracy: 8.160\n",
            "[ 18 ] Training Loss 3.664---Training Accuracy: 8.224\n",
            "[ 19 ] Training Loss 3.612---Training Accuracy: 7.812\n",
            "[ 20 ] Training Loss 3.558---Training Accuracy: 8.333\n",
            "[ 21 ] Training Loss 3.512---Training Accuracy: 8.381\n",
            "[ 22 ] Training Loss 3.464---Training Accuracy: 8.424\n",
            "[ 23 ] Training Loss 3.426---Training Accuracy: 8.073\n",
            "[ 24 ] Training Loss 3.392---Training Accuracy: 8.375\n",
            "[ 25 ] Training Loss 3.360---Training Accuracy: 8.293\n",
            "[ 26 ] Training Loss 3.327---Training Accuracy: 8.333\n",
            "[ 27 ] Training Loss 3.294---Training Accuracy: 8.705\n",
            "[ 28 ] Training Loss 3.268---Training Accuracy: 8.728\n",
            "[ 29 ] Training Loss 3.243---Training Accuracy: 8.542\n",
            "[ 30 ] Training Loss 3.220---Training Accuracy: 8.569\n",
            "[ 31 ] Training Loss 3.197---Training Accuracy: 8.398\n",
            "[ 32 ] Training Loss 3.176---Training Accuracy: 8.144\n",
            "[ 33 ] Training Loss 3.156---Training Accuracy: 8.364\n",
            "[ 34 ] Training Loss 3.137---Training Accuracy: 8.393\n",
            "[ 35 ] Training Loss 3.119---Training Accuracy: 8.333\n",
            "[ 36 ] Training Loss 3.102---Training Accuracy: 8.193\n",
            "[ 37 ] Training Loss 3.086---Training Accuracy: 8.059\n",
            "[ 38 ] Training Loss 3.071---Training Accuracy: 7.933\n",
            "[ 39 ] Training Loss 3.056---Training Accuracy: 8.203\n",
            "[ 40 ] Training Loss 3.042---Training Accuracy: 8.460\n",
            "[ 41 ] Training Loss 3.029---Training Accuracy: 8.408\n",
            "[ 42 ] Training Loss 3.016---Training Accuracy: 8.358\n",
            "[ 43 ] Training Loss 3.004---Training Accuracy: 8.381\n",
            "[ 44 ] Training Loss 2.992---Training Accuracy: 8.542\n",
            "[ 45 ] Training Loss 2.981---Training Accuracy: 8.560\n",
            "[ 46 ] Training Loss 2.971---Training Accuracy: 8.577\n",
            "[ 47 ] Training Loss 2.960---Training Accuracy: 8.724\n",
            "[ 48 ] Training Loss 2.951---Training Accuracy: 8.737\n",
            "[ 49 ] Training Loss 2.941---Training Accuracy: 8.625\n",
            "[ 50 ] Training Loss 2.932---Training Accuracy: 8.578\n",
            "[ 51 ] Training Loss 2.924---Training Accuracy: 8.474\n",
            "[ 52 ] Training Loss 2.916---Training Accuracy: 8.491\n",
            "[ 53 ] Training Loss 2.908---Training Accuracy: 8.738\n",
            "[ 54 ] Training Loss 2.900---Training Accuracy: 8.580\n",
            "[ 55 ] Training Loss 2.892---Training Accuracy: 8.538\n",
            "[ 56 ] Training Loss 2.885---Training Accuracy: 8.607\n",
            "[ 57 ] Training Loss 2.878---Training Accuracy: 8.567\n",
            "[ 58 ] Training Loss 2.872---Training Accuracy: 8.422\n",
            "[ 59 ] Training Loss 2.865---Training Accuracy: 8.438\n",
            "[ 60 ] Training Loss 2.859---Training Accuracy: 8.658\n",
            "[ 61 ] Training Loss 2.852---Training Accuracy: 8.821\n",
            "[ 62 ] Training Loss 2.847---Training Accuracy: 8.780\n",
            "[ 63 ] Training Loss 2.841---Training Accuracy: 8.691\n",
            "[ 64 ] Training Loss 2.836---Training Accuracy: 8.702\n",
            "[ 65 ] Training Loss 2.830---Training Accuracy: 8.712\n",
            "[ 66 ] Training Loss 2.825---Training Accuracy: 8.675\n",
            "[ 67 ] Training Loss 2.820---Training Accuracy: 8.686\n",
            "[ 68 ] Training Loss 2.815---Training Accuracy: 8.741\n",
            "[ 69 ] Training Loss 2.810---Training Accuracy: 8.795\n",
            "[ 70 ] Training Loss 2.805---Training Accuracy: 9.067\n",
            "[ 71 ] Training Loss 2.801---Training Accuracy: 9.028\n",
            "[ 72 ] Training Loss 2.796---Training Accuracy: 9.075\n",
            "[ 73 ] Training Loss 2.792---Training Accuracy: 9.122\n",
            "[ 74 ] Training Loss 2.788---Training Accuracy: 9.000\n",
            "[ 75 ] Training Loss 2.783---Training Accuracy: 9.128\n",
            "[ 76 ] Training Loss 2.779---Training Accuracy: 9.172\n",
            "[ 77 ] Training Loss 2.775---Training Accuracy: 9.095\n",
            "[ 78 ] Training Loss 2.772---Training Accuracy: 9.098\n",
            "[ 79 ] Training Loss 2.768---Training Accuracy: 9.023\n",
            "[ 80 ] Training Loss 2.765---Training Accuracy: 8.951\n",
            "[ 81 ] Training Loss 2.761---Training Accuracy: 8.956\n",
            "[ 82 ] Training Loss 2.758---Training Accuracy: 8.923\n",
            "[ 83 ] Training Loss 2.754---Training Accuracy: 8.929\n",
            "[ 84 ] Training Loss 2.751---Training Accuracy: 9.007\n",
            "[ 85 ] Training Loss 2.748---Training Accuracy: 9.012\n",
            "[ 86 ] Training Loss 2.745---Training Accuracy: 8.944\n",
            "[ 87 ] Training Loss 2.742---Training Accuracy: 8.949\n",
            "[ 88 ] Training Loss 2.739---Training Accuracy: 8.989\n",
            "[ 89 ] Training Loss 2.736---Training Accuracy: 8.993\n",
            "[ 90 ] Training Loss 2.733---Training Accuracy: 8.997\n",
            "[ 91 ] Training Loss 2.730---Training Accuracy: 8.967\n",
            "[ 92 ] Training Loss 2.728---Training Accuracy: 9.005\n",
            "[ 93 ] Training Loss 2.725---Training Accuracy: 9.076\n",
            "[ 94 ] Training Loss 2.722---Training Accuracy: 9.112\n",
            "[ 95 ] Training Loss 2.720---Training Accuracy: 9.115\n",
            "[ 96 ] Training Loss 2.717---Training Accuracy: 9.149\n",
            "[ 97 ] Training Loss 2.715---Training Accuracy: 9.088\n",
            "[ 98 ] Training Loss 2.712---Training Accuracy: 9.059\n",
            "[ 99 ] Training Loss 2.710---Training Accuracy: 9.062\n",
            "[ 100 ] Training Loss 2.707---Training Accuracy: 9.066\n",
            "[ 101 ] Training Loss 2.705---Training Accuracy: 9.038\n",
            "[ 102 ] Training Loss 2.703---Training Accuracy: 9.011\n",
            "[ 103 ] Training Loss 2.701---Training Accuracy: 9.044\n",
            "[ 104 ] Training Loss 2.698---Training Accuracy: 9.137\n",
            "[ 105 ] Training Loss 2.696---Training Accuracy: 9.080\n",
            "[ 106 ] Training Loss 2.694---Training Accuracy: 9.083\n",
            "[ 107 ] Training Loss 2.692---Training Accuracy: 9.086\n",
            "[ 108 ] Training Loss 2.690---Training Accuracy: 9.117\n",
            "[ 109 ] Training Loss 2.688---Training Accuracy: 9.091\n",
            "[ 110 ] Training Loss 2.686---Training Accuracy: 9.122\n",
            "[ 111 ] Training Loss 2.684---Training Accuracy: 9.124\n",
            "[ 112 ] Training Loss 2.683---Training Accuracy: 9.154\n",
            "[ 113 ] Training Loss 2.681---Training Accuracy: 9.183\n",
            "[ 114 ] Training Loss 2.679---Training Accuracy: 9.130\n",
            "[ 115 ] Training Loss 2.677---Training Accuracy: 9.159\n",
            "[ 116 ] Training Loss 2.675---Training Accuracy: 9.241\n",
            "[ 117 ] Training Loss 2.674---Training Accuracy: 9.243\n",
            "[ 118 ] Training Loss 2.672---Training Accuracy: 9.270\n",
            "[ 119 ] Training Loss 2.670---Training Accuracy: 9.297\n",
            "[ 120 ] Training Loss 2.669---Training Accuracy: 9.220\n",
            "[ 121 ] Training Loss 2.667---Training Accuracy: 9.247\n",
            "[ 122 ] Training Loss 2.666---Training Accuracy: 9.273\n",
            "[ 123 ] Training Loss 2.664---Training Accuracy: 9.199\n",
            "[ 124 ] Training Loss 2.663---Training Accuracy: 9.175\n",
            "[ 125 ] Training Loss 2.661---Training Accuracy: 9.226\n",
            "[ 126 ] Training Loss 2.659---Training Accuracy: 9.227\n",
            "[ 127 ] Training Loss 2.658---Training Accuracy: 9.277\n",
            "[ 128 ] Training Loss 2.657---Training Accuracy: 9.302\n",
            "[ 129 ] Training Loss 2.655---Training Accuracy: 9.303\n",
            "[ 130 ] Training Loss 2.654---Training Accuracy: 9.327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4009ac66a863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-4009ac66a863>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# # When using GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}