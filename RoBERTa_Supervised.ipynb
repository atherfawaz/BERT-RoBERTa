{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBERTa Supervised.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNLskAKFSBv8LZtyfW33Ppl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atherfawaz/BERT-Supervised/blob/master/RoBERTa_Supervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as3HYaN9NdGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "6ace5d6c-f79b-43e3-9754-38001bde0bbf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/drive/My Drive/Ebryx/blogs_train"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/Ebryx/blogs_train'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgFaA1_yNwta",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "286f8942-c2dd-4919-ecf8-c1c242e74896"
      },
      "source": [
        "# Code for TPU packages install\n",
        "!curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5115  100  5115    0     0  26640      0 --:--:-- --:--:-- --:--:-- 26640\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200515 ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (49.2.0)\n",
            "Uninstalling torch-1.6.0+cu101:\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.12.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.10)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "Done updating TPU runtime\n",
            "  Successfully uninstalled torch-1.6.0+cu101\n",
            "Uninstalling torchvision-0.7.0+cu101:\n",
            "  Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "| [1 files][ 91.0 MiB/ 91.0 MiB]                                                \n",
            "Operation completed over 1 objects/91.0 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][119.5 MiB/119.5 MiB]                                                \n",
            "Operation completed over 1 objects/119.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.3 MiB/  2.3 MiB]                                                \n",
            "Operation completed over 1 objects/2.3 MiB.                                      \n",
            "Processing ./torch-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200515) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200515) (0.16.0)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.6.0a0+bf2bbd9\n",
            "Processing ./torch_xla-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+2b2085a\n",
            "Processing ./torchvision-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (7.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (1.6.0a0+bf2bbd9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200515) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.7.0a0+a6073f0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopenblas-dev is already the newest version (0.2.20+ds-4).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (362 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyjyqETWN_aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q transformers\n",
        "\n",
        "# Importing stock libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Importing lackages from our NLP-Hugging Package\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaTokenizerFast, RobertaForSequenceClassification"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmpJLL_NOO9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TPU\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "from sklearn.utils import shuffle\n",
        "device = xm.xla_device()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AwJThPGOgfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "82f7fd3e-4f36-4099-8392-42340713959b"
      },
      "source": [
        "%cd /content/gdrive/My Drive/Ebryx/blogs_train\n",
        "# Creating the dataset and dataloader for the neural network\n",
        "df = pd.read_csv('/content/gdrive/My Drive/Ebryx/dataset.csv')\n",
        "df = df[['Post', 'Sign', 'Gender']]\n",
        "\n",
        "encode_dict = {}\n",
        "\n",
        "def encode_cat(x):\n",
        "    if x not in encode_dict.keys():\n",
        "      encode_dict[x]=len(encode_dict)\n",
        "    return encode_dict[x]\n",
        "\n",
        "df['Sign'] = df['Sign'].apply(lambda x: encode_cat(x))\n",
        "df['Post'] = df['Post'].str.replace('\\r','').str.replace('\\t','').str.replace('\\xa0', '')\n",
        "\n",
        "df = df [['Post', 'Sign']]\n",
        "\n",
        "df = shuffle(df)\n",
        "df.head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Ebryx/blogs_train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Post</th>\n",
              "      <th>Sign</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>72872</th>\n",
              "      <td>\"The difference between us and nerds is...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20097</th>\n",
              "      <td>nothing much to say today... its just...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354659</th>\n",
              "      <td>Today was sort of fun. I had a re...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345094</th>\n",
              "      <td>Major prayer request: This is still jus...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68858</th>\n",
              "      <td>grr. Ive been in a bad mood for a few d...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29585</th>\n",
              "      <td>Jason Kan is a nice guy...seriously. He...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82489</th>\n",
              "      <td>urlLink    I like this contemplative h...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76804</th>\n",
              "      <td>Oh, I wouldn't consider myself all ...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56050</th>\n",
              "      <td>Look Kevin I'm really tired of yo...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325670</th>\n",
              "      <td>The story of God ordering Abraham to sa...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     Post  Sign\n",
              "72872          \"The difference between us and nerds is...    11\n",
              "20097            nothing much to say today... its just...     5\n",
              "354659               Today was sort of fun. I had a re...     0\n",
              "345094         Major prayer request: This is still jus...     2\n",
              "68858          grr. Ive been in a bad mood for a few d...     4\n",
              "29585          Jason Kan is a nice guy...seriously. He...     0\n",
              "82489           urlLink    I like this contemplative h...     8\n",
              "76804              Oh, I wouldn't consider myself all ...     7\n",
              "56050                Look Kevin I'm really tired of yo...     7\n",
              "325670         The story of God ordering Abraham to sa...     4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_An5jwgLOr_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c4cfa611-7945-4a8b-df4a-2105014c2d71"
      },
      "source": [
        "# Creating a CustomDataset class that is used to read the updated dataframe and tokenize the text. \n",
        "# The class is used in the return_dataloader function\n",
        "\n",
        "class Triage(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        post = str(self.data.Post[index])\n",
        "        post = \" \".join(post.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            post,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.data.Sign[index], dtype=torch.float)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state=200)\n",
        "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "            'shuffle': True,\n",
        "            'num_workers': 1\n",
        "            }\n",
        "\n",
        "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 1\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "validation_loader = DataLoader(testing_set, **val_params)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (380720, 2)\n",
            "TRAIN Dataset: (304576, 2)\n",
            "TEST Dataset: (76144, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1NFa3NHQW9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of roberta to get the final output for the model. \n",
        "\n",
        "class ModelClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelClass, self).__init__()\n",
        "        self.model_layer = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 12)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.model_layer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "# Function to return model based on the defination of Model Class\n",
        "def return_model(device):\n",
        "    model = ModelClass()\n",
        "    model = model.to(device)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Function to calcuate the accuracy of the model\n",
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vQzyNLWRX6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a670d55-908b-433f-c123-e63c44b9ae31"
      },
      "source": [
        "model = ModelClass()\n",
        "model.to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  tr_loss = 0\n",
        "  n_correct = 0\n",
        "  nb_tr_steps = 0\n",
        "  nb_tr_examples = 0\n",
        "  model.train()\n",
        "  for i,data in enumerate(training_loader, 0):\n",
        "      ids = data['ids'].to(device, dtype = torch.long)\n",
        "      mask = data['mask'].to(device, dtype = torch.long)\n",
        "      targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "      outputs = model(ids, mask)\n",
        "      loss = loss_function(outputs, targets)\n",
        "      tr_loss += loss.item()\n",
        "      big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "      n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "      nb_tr_steps += 1\n",
        "      nb_tr_examples+=targets.size(0)\n",
        "      \n",
        "      if i%5==0:\n",
        "          loss_step = tr_loss/nb_tr_steps\n",
        "          accu_step = (n_correct*100)/nb_tr_examples \n",
        "          print(f\"[ {i} ] Loss: {loss_step:.3f} | Accuracy: {accu_step:.3f}\")\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      # # When using GPU\n",
        "      #optimizer.step()\n",
        "\n",
        "       # When using TPU\n",
        "      xm.optimizer_step(optimizer)\n",
        "      xm.mark_step()\n",
        "\n",
        "  print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "  epoch_loss = tr_loss/nb_tr_steps\n",
        "  epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "  print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "  print(f\"Training Accuracy Epoch: {epoch_accu}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0 ] Loss: 2.523 | Accuracy: 6.250\n",
            "[ 5 ] Loss: 2.513 | Accuracy: 7.292\n",
            "[ 10 ] Loss: 2.498 | Accuracy: 6.250\n",
            "[ 15 ] Loss: 2.505 | Accuracy: 6.250\n",
            "[ 20 ] Loss: 2.503 | Accuracy: 6.250\n",
            "[ 25 ] Loss: 2.497 | Accuracy: 7.212\n",
            "[ 30 ] Loss: 2.499 | Accuracy: 6.855\n",
            "[ 35 ] Loss: 2.500 | Accuracy: 6.944\n",
            "[ 40 ] Loss: 2.496 | Accuracy: 8.079\n",
            "[ 45 ] Loss: 2.492 | Accuracy: 8.288\n",
            "[ 50 ] Loss: 2.491 | Accuracy: 8.333\n",
            "[ 55 ] Loss: 2.491 | Accuracy: 8.371\n",
            "[ 60 ] Loss: 2.490 | Accuracy: 8.299\n",
            "[ 65 ] Loss: 2.489 | Accuracy: 8.523\n",
            "[ 70 ] Loss: 2.488 | Accuracy: 8.627\n",
            "[ 75 ] Loss: 2.488 | Accuracy: 8.635\n",
            "[ 80 ] Loss: 2.488 | Accuracy: 8.719\n",
            "[ 85 ] Loss: 2.487 | Accuracy: 8.576\n",
            "[ 90 ] Loss: 2.488 | Accuracy: 8.791\n",
            "[ 95 ] Loss: 2.487 | Accuracy: 8.724\n",
            "[ 100 ] Loss: 2.487 | Accuracy: 8.787\n",
            "[ 105 ] Loss: 2.486 | Accuracy: 8.903\n",
            "[ 110 ] Loss: 2.487 | Accuracy: 8.840\n",
            "[ 115 ] Loss: 2.487 | Accuracy: 8.890\n",
            "[ 120 ] Loss: 2.487 | Accuracy: 9.091\n",
            "[ 125 ] Loss: 2.486 | Accuracy: 9.028\n",
            "[ 130 ] Loss: 2.486 | Accuracy: 9.113\n",
            "[ 135 ] Loss: 2.485 | Accuracy: 9.099\n",
            "[ 140 ] Loss: 2.485 | Accuracy: 8.998\n",
            "[ 145 ] Loss: 2.484 | Accuracy: 9.118\n",
            "[ 150 ] Loss: 2.484 | Accuracy: 9.230\n",
            "[ 155 ] Loss: 2.484 | Accuracy: 9.375\n",
            "[ 160 ] Loss: 2.484 | Accuracy: 9.356\n",
            "[ 165 ] Loss: 2.484 | Accuracy: 9.413\n",
            "[ 170 ] Loss: 2.484 | Accuracy: 9.503\n",
            "[ 175 ] Loss: 2.485 | Accuracy: 9.375\n",
            "[ 180 ] Loss: 2.485 | Accuracy: 9.530\n",
            "[ 185 ] Loss: 2.484 | Accuracy: 9.711\n",
            "[ 190 ] Loss: 2.485 | Accuracy: 9.653\n",
            "[ 195 ] Loss: 2.485 | Accuracy: 9.694\n",
            "[ 200 ] Loss: 2.485 | Accuracy: 9.764\n",
            "[ 205 ] Loss: 2.485 | Accuracy: 9.739\n",
            "[ 210 ] Loss: 2.484 | Accuracy: 9.805\n",
            "[ 215 ] Loss: 2.484 | Accuracy: 9.780\n",
            "[ 220 ] Loss: 2.483 | Accuracy: 9.700\n",
            "[ 225 ] Loss: 2.483 | Accuracy: 9.624\n",
            "[ 230 ] Loss: 2.484 | Accuracy: 9.578\n",
            "[ 235 ] Loss: 2.483 | Accuracy: 9.719\n",
            "[ 240 ] Loss: 2.484 | Accuracy: 9.751\n",
            "[ 245 ] Loss: 2.483 | Accuracy: 9.731\n",
            "[ 250 ] Loss: 2.484 | Accuracy: 9.661\n",
            "[ 255 ] Loss: 2.484 | Accuracy: 9.766\n",
            "[ 260 ] Loss: 2.484 | Accuracy: 9.842\n",
            "[ 265 ] Loss: 2.484 | Accuracy: 9.798\n",
            "[ 270 ] Loss: 2.484 | Accuracy: 9.756\n",
            "[ 275 ] Loss: 2.485 | Accuracy: 9.669\n",
            "[ 280 ] Loss: 2.484 | Accuracy: 9.698\n",
            "[ 285 ] Loss: 2.484 | Accuracy: 9.659\n",
            "[ 290 ] Loss: 2.484 | Accuracy: 9.643\n",
            "[ 295 ] Loss: 2.485 | Accuracy: 9.565\n",
            "[ 300 ] Loss: 2.485 | Accuracy: 9.572\n",
            "[ 305 ] Loss: 2.484 | Accuracy: 9.600\n",
            "[ 310 ] Loss: 2.484 | Accuracy: 9.707\n",
            "[ 315 ] Loss: 2.484 | Accuracy: 9.691\n",
            "[ 320 ] Loss: 2.484 | Accuracy: 9.774\n",
            "[ 325 ] Loss: 2.484 | Accuracy: 9.816\n",
            "[ 330 ] Loss: 2.484 | Accuracy: 9.762\n",
            "[ 335 ] Loss: 2.484 | Accuracy: 9.710\n",
            "[ 340 ] Loss: 2.484 | Accuracy: 9.677\n",
            "[ 345 ] Loss: 2.484 | Accuracy: 9.646\n",
            "[ 350 ] Loss: 2.484 | Accuracy: 9.615\n",
            "[ 355 ] Loss: 2.484 | Accuracy: 9.691\n",
            "[ 360 ] Loss: 2.484 | Accuracy: 9.730\n",
            "[ 365 ] Loss: 2.484 | Accuracy: 9.665\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}