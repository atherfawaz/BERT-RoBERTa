{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBERTa Supervised.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/l7pN5nAYTby21cnqOaFV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atherfawaz/BERT-Supervised/blob/master/RoBERTa_Supervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as3HYaN9NdGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "6ace5d6c-f79b-43e3-9754-38001bde0bbf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/drive/My Drive/Ebryx/blogs_train"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/Ebryx/blogs_train'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgFaA1_yNwta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code for TPU packages install\n",
        "!curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyjyqETWN_aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q transformers\n",
        "\n",
        "# Importing stock libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Importing lackages from our NLP-Hugging Package\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaTokenizerFast, RobertaForSequenceClassification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmpJLL_NOO9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TPU\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "from sklearn.utils import shuffle\n",
        "device = xm.xla_device()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AwJThPGOgfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "9bbbdc7d-cde7-498f-b18b-153da03c29bc"
      },
      "source": [
        "%cd /content/gdrive/My Drive/Ebryx/blogs_train\n",
        "# Creating the dataset and dataloader for the neural network\n",
        "df = pd.read_csv('/content/gdrive/My Drive/Ebryx/dataset.csv')\n",
        "df = df[['Post', 'Sign', 'Gender']]\n",
        "\n",
        "encode_dict = {}\n",
        "\n",
        "def encode_cat(x):\n",
        "    if x not in encode_dict.keys():\n",
        "      encode_dict[x]=len(encode_dict)\n",
        "    return encode_dict[x]\n",
        "\n",
        "df['Sign'] = df['Sign'].apply(lambda x: encode_cat(x))\n",
        "df['Post'] = df['Post'].str.replace('\\r','').str.replace('\\t','').str.replace('\\xa0', '').str.replace('urlLink', '')\n",
        "df['Count'] = df['Post'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "average_words = df['Count'].mean()\n",
        "print('Average Words: ', average_words)\n",
        "\n",
        "df = df [['Post', 'Sign']]\n",
        "\n",
        "df = shuffle(df)\n",
        "df.head(10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Ebryx/blogs_train\n",
            "Average Words:  201.2948833788611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Post</th>\n",
              "      <th>Sign</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>100817</th>\n",
              "      <td>waiting for mommy to get home so i can ...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23416</th>\n",
              "      <td>It's named as \"Let's put the piece...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63355</th>\n",
              "      <td>Pete Guither  explains that Bush plan...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123586</th>\n",
              "      <td>I need to speak to the Lindse...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300949</th>\n",
              "      <td>The Talk   N= Natalie--9 year old I ba...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285056</th>\n",
              "      <td>2004-04-05 12:51:00; I think I'm gettin...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162268</th>\n",
              "      <td>God I hate Sunday's they are ...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122942</th>\n",
              "      <td>Wow kids...  Everyone's posti...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256064</th>\n",
              "      <td>so i went for a walk with Carrie after ...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298381</th>\n",
              "      <td>Holy shit I really haven't blogge...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     Post  Sign\n",
              "100817         waiting for mommy to get home so i can ...    10\n",
              "23416               It's named as \"Let's put the piece...     6\n",
              "63355            Pete Guither  explains that Bush plan...     0\n",
              "123586                   I need to speak to the Lindse...     4\n",
              "300949          The Talk   N= Natalie--9 year old I ba...     0\n",
              "285056         2004-04-05 12:51:00; I think I'm gettin...     4\n",
              "162268                   God I hate Sunday's they are ...     8\n",
              "122942                   Wow kids...  Everyone's posti...     4\n",
              "256064         so i went for a walk with Carrie after ...    10\n",
              "298381               Holy shit I really haven't blogge...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_An5jwgLOr_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e16fa682-36c5-4787-bd5b-5870852e16d3"
      },
      "source": [
        "# Creating a CustomDataset class that is used to read the updated dataframe and tokenize the text. \n",
        "# The class is used in the return_dataloader function\n",
        "\n",
        "class Triage(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        post = str(self.data.Post[index])\n",
        "        post = \" \".join(post.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            post,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.data.Sign[index], dtype=torch.float)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state=200)\n",
        "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "            'shuffle': True,\n",
        "            'num_workers': 1\n",
        "            }\n",
        "\n",
        "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 1\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "validation_loader = DataLoader(testing_set, **val_params)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (380720, 2)\n",
            "TRAIN Dataset: (304576, 2)\n",
            "TEST Dataset: (76144, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1NFa3NHQW9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of roberta to get the final output for the model. \n",
        "\n",
        "class ModelClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelClass, self).__init__()\n",
        "        self.model_layer = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 12)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.model_layer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "# Function to return model based on the defination of Model Class\n",
        "def return_model(device):\n",
        "    model = ModelClass()\n",
        "    model = model.to(device)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Function to calcuate the accuracy of the model\n",
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-ChckoKKPFk",
        "colab_type": "text"
      },
      "source": [
        "# Epoch 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vQzyNLWRX6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a4f314ca-2cc1-4a83-bdb2-c5565a3e9c9b"
      },
      "source": [
        "#model = ModelClass()\n",
        "#model.to(device)\n",
        "\n",
        "#loss_function = torch.nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  steps = 0\n",
        "  n_tr_correct, n_val_correct = 0, 0\n",
        "  tr_loss, val_loss = 0, 0\n",
        "  nb_tr_examples, nb_val_examples = 0, 0\n",
        "  for (i, data), (j, val_data) in zip(enumerate(training_loader, 0), enumerate(validation_loader, 0)):\n",
        "      model.train()\n",
        "      ids = data['ids'].to(device, dtype = torch.long)\n",
        "      mask = data['mask'].to(device, dtype = torch.long)\n",
        "      targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "      outputs = model(ids, mask).squeeze()\n",
        "      optimizer.zero_grad()\n",
        "      loss = loss_function(outputs, targets)\n",
        "      big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "      \n",
        "      n_tr_correct += calcuate_accu(big_idx, targets)\n",
        "      nb_tr_examples += targets.size(0)\n",
        "\n",
        "      tr_loss += loss.item()\n",
        "      \n",
        "      if i%100==0:\n",
        "        loss_step = tr_loss/100\n",
        "        train_acc = n_tr_correct/nb_tr_examples\n",
        "        tr_loss, n_tr_correct, nb_tr_examples = 0, 0, 0\n",
        "        print(f\"[{i}] train_loss: {loss_step:.3f} train_acc: {train_acc:.3f}\", end=\"    \") \n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      \n",
        "      # # When using GPU or GPU\n",
        "      # optimizer.step()\n",
        "      \n",
        "      # When using TPU\n",
        "      xm.optimizer_step(optimizer)\n",
        "      xm.mark_step()\n",
        "\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          ids = val_data['ids'].to(device, dtype = torch.long)\n",
        "          mask = val_data['mask'].to(device, dtype = torch.long)\n",
        "          targets = val_data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "          outputs = model(ids, mask).squeeze()\n",
        "          loss = loss_function(outputs, targets)\n",
        "          big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "          \n",
        "          n_val_correct += calcuate_accu(big_idx, targets)\n",
        "          nb_val_examples += targets.size(0)\n",
        "\n",
        "          val_loss += loss.item()\n",
        "          \n",
        "          if j%100==0:\n",
        "            val_step = val_loss/100\n",
        "            val_acc = n_val_correct/nb_val_examples\n",
        "            val_loss, n_val_correct, nb_val_examples = 0, 0, 0\n",
        "            print(f\"val_loss: {val_step:.3f} val_acc : {val_acc:.3f}\") "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0] train_loss: 0.025 train_acc: 0.125    val_loss: 0.025 val_acc : 0.000\n",
            "[100] train_loss: 2.483 train_acc: 0.094    val_loss: 2.473 val_acc : 0.110\n",
            "[200] train_loss: 2.481 train_acc: 0.095    val_loss: 2.475 val_acc : 0.111\n",
            "[300] train_loss: 2.479 train_acc: 0.100    val_loss: 2.482 val_acc : 0.087\n",
            "[400] train_loss: 2.477 train_acc: 0.094    val_loss: 2.483 val_acc : 0.101\n",
            "[500] train_loss: 2.480 train_acc: 0.098    val_loss: 2.478 val_acc : 0.101\n",
            "[600] train_loss: 2.481 train_acc: 0.094    val_loss: 2.464 val_acc : 0.128\n",
            "[700] train_loss: 2.477 train_acc: 0.104    val_loss: 2.482 val_acc : 0.080\n",
            "[800] train_loss: 2.476 train_acc: 0.107    val_loss: 2.470 val_acc : 0.105\n",
            "[900] train_loss: 2.475 train_acc: 0.110    val_loss: 2.481 val_acc : 0.081\n",
            "[1000] train_loss: 2.471 train_acc: 0.108    val_loss: 2.483 val_acc : 0.087\n",
            "[1100] train_loss: 2.468 train_acc: 0.102    val_loss: 2.466 val_acc : 0.110\n",
            "[1200] train_loss: 2.464 train_acc: 0.106    val_loss: 2.485 val_acc : 0.094\n",
            "[1300] train_loss: 2.470 train_acc: 0.104    val_loss: 2.477 val_acc : 0.115\n",
            "[1400] train_loss: 2.466 train_acc: 0.108    val_loss: 2.441 val_acc : 0.155\n",
            "[1500] train_loss: 2.455 train_acc: 0.116    val_loss: 2.449 val_acc : 0.125\n",
            "[1600] train_loss: 2.458 train_acc: 0.117    val_loss: 2.464 val_acc : 0.102\n",
            "[1700] train_loss: 2.452 train_acc: 0.120    val_loss: 2.430 val_acc : 0.144\n",
            "[1800] train_loss: 2.445 train_acc: 0.124    val_loss: 2.453 val_acc : 0.133\n",
            "[1900] train_loss: 2.452 train_acc: 0.123    val_loss: 2.432 val_acc : 0.138\n",
            "[2000] train_loss: 2.440 train_acc: 0.122    val_loss: 2.434 val_acc : 0.131\n",
            "[2100] train_loss: 2.442 train_acc: 0.123    val_loss: 2.449 val_acc : 0.124\n",
            "[2200] train_loss: 2.444 train_acc: 0.129    val_loss: 2.438 val_acc : 0.136\n",
            "[2300] train_loss: 2.430 train_acc: 0.138    val_loss: 2.449 val_acc : 0.124\n",
            "[2400] train_loss: 2.431 train_acc: 0.136    val_loss: 2.432 val_acc : 0.131\n",
            "[2500] train_loss: 2.431 train_acc: 0.128    val_loss: 2.416 val_acc : 0.125\n",
            "[2600] train_loss: 2.422 train_acc: 0.138    val_loss: 2.416 val_acc : 0.134\n",
            "[2700] train_loss: 2.431 train_acc: 0.136    val_loss: 2.437 val_acc : 0.120\n",
            "[2800] train_loss: 2.421 train_acc: 0.133    val_loss: 2.433 val_acc : 0.149\n",
            "[2900] train_loss: 2.418 train_acc: 0.139    val_loss: 2.403 val_acc : 0.156\n",
            "[3000] train_loss: 2.422 train_acc: 0.141    val_loss: 2.403 val_acc : 0.149\n",
            "[3100] train_loss: 2.414 train_acc: 0.149    val_loss: 2.377 val_acc : 0.166\n",
            "[3200] train_loss: 2.414 train_acc: 0.148    val_loss: 2.394 val_acc : 0.158\n",
            "[3300] train_loss: 2.421 train_acc: 0.134    val_loss: 2.432 val_acc : 0.133\n",
            "[3400] train_loss: 2.395 train_acc: 0.143    val_loss: 2.387 val_acc : 0.169\n",
            "[3500] train_loss: 2.400 train_acc: 0.150    val_loss: 2.372 val_acc : 0.150\n",
            "[3600] train_loss: 2.404 train_acc: 0.158    val_loss: 2.372 val_acc : 0.171\n",
            "[3700] train_loss: 2.392 train_acc: 0.156    val_loss: 2.377 val_acc : 0.145\n",
            "[3800] train_loss: 2.403 train_acc: 0.156    val_loss: 2.351 val_acc : 0.166\n",
            "[3900] train_loss: 2.391 train_acc: 0.158    val_loss: 2.377 val_acc : 0.147\n",
            "[4000] train_loss: 2.383 train_acc: 0.160    val_loss: 2.363 val_acc : 0.177\n",
            "[4100] train_loss: 2.393 train_acc: 0.160    val_loss: 2.365 val_acc : 0.174\n",
            "[4200] train_loss: 2.386 train_acc: 0.161    val_loss: 2.381 val_acc : 0.186\n",
            "[4300] train_loss: 2.378 train_acc: 0.161    val_loss: 2.344 val_acc : 0.166\n",
            "[4400] train_loss: 2.380 train_acc: 0.153    val_loss: 2.359 val_acc : 0.169\n",
            "[4500] train_loss: 2.374 train_acc: 0.168    val_loss: 2.363 val_acc : 0.179\n",
            "[4600] train_loss: 2.363 train_acc: 0.173    val_loss: 2.365 val_acc : 0.146\n",
            "[4700] train_loss: 2.361 train_acc: 0.177    val_loss: 2.336 val_acc : 0.188\n",
            "[4800] train_loss: 2.367 train_acc: 0.168    val_loss: 2.337 val_acc : 0.182\n",
            "[4900] train_loss: 2.354 train_acc: 0.179    val_loss: 2.311 val_acc : 0.196\n",
            "[5000] train_loss: 2.352 train_acc: 0.180    val_loss: 2.352 val_acc : 0.169\n",
            "[5100] train_loss: 2.358 train_acc: 0.173    val_loss: 2.324 val_acc : 0.201\n",
            "[5200] train_loss: 2.368 train_acc: 0.176    val_loss: 2.370 val_acc : 0.168\n",
            "[5300] train_loss: 2.349 train_acc: 0.173    val_loss: 2.347 val_acc : 0.189\n",
            "[5400] train_loss: 2.367 train_acc: 0.179    val_loss: 2.329 val_acc : 0.184\n",
            "[5500] train_loss: 2.335 train_acc: 0.188    val_loss: 2.356 val_acc : 0.188\n",
            "[5600] train_loss: 2.349 train_acc: 0.180    val_loss: 2.326 val_acc : 0.181\n",
            "[5700] train_loss: 2.327 train_acc: 0.186    val_loss: 2.307 val_acc : 0.205\n",
            "[5800] train_loss: 2.341 train_acc: 0.180    val_loss: 2.343 val_acc : 0.190\n",
            "[5900] train_loss: 2.324 train_acc: 0.189    val_loss: 2.310 val_acc : 0.205\n",
            "[6000] train_loss: 2.328 train_acc: 0.189    val_loss: 2.315 val_acc : 0.186\n",
            "[6100] train_loss: 2.326 train_acc: 0.191    val_loss: 2.324 val_acc : 0.196\n",
            "[6200] train_loss: 2.317 train_acc: 0.194    val_loss: 2.286 val_acc : 0.185\n",
            "[6300] train_loss: 2.309 train_acc: 0.203    val_loss: 2.319 val_acc : 0.191\n",
            "[6400] train_loss: 2.305 train_acc: 0.194    val_loss: 2.276 val_acc : 0.209\n",
            "[6500] train_loss: 2.306 train_acc: 0.193    val_loss: 2.310 val_acc : 0.195\n",
            "[6600] train_loss: 2.313 train_acc: 0.188    val_loss: 2.282 val_acc : 0.200\n",
            "[6700] train_loss: 2.308 train_acc: 0.195    val_loss: 2.271 val_acc : 0.219\n",
            "[6800] train_loss: 2.315 train_acc: 0.188    val_loss: 2.303 val_acc : 0.196\n",
            "[6900] train_loss: 2.301 train_acc: 0.193    val_loss: 2.286 val_acc : 0.217\n",
            "[7000] train_loss: 2.307 train_acc: 0.202    val_loss: 2.323 val_acc : 0.194\n",
            "[7100] train_loss: 2.286 train_acc: 0.203    val_loss: 2.271 val_acc : 0.211\n",
            "[7200] train_loss: 2.282 train_acc: 0.211    val_loss: 2.249 val_acc : 0.210\n",
            "[7300] train_loss: 2.295 train_acc: 0.207    val_loss: 2.284 val_acc : 0.211\n",
            "[7400] train_loss: 2.291 train_acc: 0.209    val_loss: 2.244 val_acc : 0.194\n",
            "[7500] train_loss: 2.285 train_acc: 0.205    val_loss: 2.241 val_acc : 0.210\n",
            "[7600] train_loss: 2.281 train_acc: 0.206    val_loss: 2.241 val_acc : 0.209\n",
            "[7700] train_loss: 2.291 train_acc: 0.213    val_loss: 2.315 val_acc : 0.199\n",
            "[7800] train_loss: 2.281 train_acc: 0.213    val_loss: 2.278 val_acc : 0.214\n",
            "[7900] train_loss: 2.277 train_acc: 0.214    val_loss: 2.227 val_acc : 0.226\n",
            "[8000] train_loss: 2.276 train_acc: 0.212    val_loss: 2.281 val_acc : 0.212\n",
            "[8100] train_loss: 2.292 train_acc: 0.203    val_loss: 2.300 val_acc : 0.194\n",
            "[8200] train_loss: 2.285 train_acc: 0.209    val_loss: 2.297 val_acc : 0.203\n",
            "[8300] train_loss: 2.256 train_acc: 0.215    val_loss: 2.245 val_acc : 0.236\n",
            "[8400] train_loss: 2.267 train_acc: 0.223    val_loss: 2.229 val_acc : 0.226\n",
            "[8500] train_loss: 2.284 train_acc: 0.208    val_loss: 2.263 val_acc : 0.228\n",
            "[8600] train_loss: 2.261 train_acc: 0.217    val_loss: 2.230 val_acc : 0.229\n",
            "[8700] train_loss: 2.275 train_acc: 0.210    val_loss: 2.215 val_acc : 0.231\n",
            "[8800] train_loss: 2.258 train_acc: 0.215    val_loss: 2.252 val_acc : 0.228\n",
            "[8900] train_loss: 2.244 train_acc: 0.237    val_loss: 2.237 val_acc : 0.234\n",
            "[9000] train_loss: 2.271 train_acc: 0.215    val_loss: 2.246 val_acc : 0.235\n",
            "[9100] train_loss: 2.246 train_acc: 0.225    val_loss: 2.238 val_acc : 0.221\n",
            "[9200] train_loss: 2.240 train_acc: 0.222    val_loss: 2.289 val_acc : 0.209\n",
            "[9300] train_loss: 2.257 train_acc: 0.217    val_loss: 2.258 val_acc : 0.229\n",
            "[9400] train_loss: 2.258 train_acc: 0.226    val_loss: 2.229 val_acc : 0.211\n",
            "[9500] train_loss: 2.237 train_acc: 0.226    val_loss: 2.259 val_acc : 0.223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNXSP9cfKIz2",
        "colab_type": "text"
      },
      "source": [
        "# Epoch 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LaTJNQ1KBKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "4edbb65a-6167-4ed7-a2b0-bfe029ceac47"
      },
      "source": [
        "steps = 0\n",
        "n_tr_correct, n_val_correct = 0, 0\n",
        "tr_loss, val_loss = 0, 0\n",
        "nb_tr_examples, nb_val_examples = 0, 0\n",
        "for (i, data), (j, val_data) in zip(enumerate(training_loader, 0), enumerate(validation_loader, 0)):\n",
        "    model.train()\n",
        "    ids = data['ids'].to(device, dtype = torch.long)\n",
        "    mask = data['mask'].to(device, dtype = torch.long)\n",
        "    targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "    outputs = model(ids, mask).squeeze()\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_function(outputs, targets)\n",
        "    big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "    \n",
        "    n_tr_correct += calcuate_accu(big_idx, targets)\n",
        "    nb_tr_examples += targets.size(0)\n",
        "\n",
        "    tr_loss += loss.item()\n",
        "    \n",
        "    if i%100==0:\n",
        "      loss_step = tr_loss/100\n",
        "      train_acc = n_tr_correct/nb_tr_examples\n",
        "      tr_loss, n_tr_correct, nb_tr_examples = 0, 0, 0\n",
        "      print(f\"[{i}] train_loss: {loss_step:.3f} train_acc: {train_acc:.3f}\", end=\"    \") \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    \n",
        "    # # When using GPU or GPU\n",
        "    # optimizer.step()\n",
        "    \n",
        "    # When using TPU\n",
        "    xm.optimizer_step(optimizer)\n",
        "    xm.mark_step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        ids = val_data['ids'].to(device, dtype = torch.long)\n",
        "        mask = val_data['mask'].to(device, dtype = torch.long)\n",
        "        targets = val_data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask).squeeze()\n",
        "        loss = loss_function(outputs, targets)\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        \n",
        "        n_val_correct += calcuate_accu(big_idx, targets)\n",
        "        nb_val_examples += targets.size(0)\n",
        "\n",
        "        val_loss += loss.item()\n",
        "        \n",
        "        if j%100==0:\n",
        "          val_step = val_loss/100\n",
        "          val_acc = n_val_correct/nb_val_examples\n",
        "          val_loss, n_val_correct, nb_val_examples = 0, 0, 0\n",
        "          print(f\"val_loss: {val_step:.3f} val_acc : {val_acc:.3f}\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0] train_loss: 0.023 train_acc: 0.156    val_loss: 0.018 val_acc : 0.375\n",
            "[100] train_loss: 2.213 train_acc: 0.237    val_loss: 2.258 val_acc : 0.239\n",
            "[200] train_loss: 2.210 train_acc: 0.242    val_loss: 2.228 val_acc : 0.214\n",
            "[300] train_loss: 2.222 train_acc: 0.243    val_loss: 2.261 val_acc : 0.210\n",
            "[400] train_loss: 2.204 train_acc: 0.235    val_loss: 2.252 val_acc : 0.230\n",
            "[500] train_loss: 2.208 train_acc: 0.239    val_loss: 2.208 val_acc : 0.239\n",
            "[600] train_loss: 2.206 train_acc: 0.241    val_loss: 2.268 val_acc : 0.225\n",
            "[700] train_loss: 2.167 train_acc: 0.248    val_loss: 2.262 val_acc : 0.211\n",
            "[800] train_loss: 2.197 train_acc: 0.245    val_loss: 2.206 val_acc : 0.241\n",
            "[900] train_loss: 2.198 train_acc: 0.264    val_loss: 2.173 val_acc : 0.249\n",
            "[1000] train_loss: 2.186 train_acc: 0.247    val_loss: 2.156 val_acc : 0.264\n",
            "[1100] train_loss: 2.205 train_acc: 0.238    val_loss: 2.176 val_acc : 0.250\n",
            "[1200] train_loss: 2.174 train_acc: 0.250    val_loss: 2.254 val_acc : 0.215\n",
            "[1300] train_loss: 2.166 train_acc: 0.255    val_loss: 2.223 val_acc : 0.240\n",
            "[1400] train_loss: 2.177 train_acc: 0.253    val_loss: 2.212 val_acc : 0.253\n",
            "[1500] train_loss: 2.188 train_acc: 0.253    val_loss: 2.187 val_acc : 0.260\n",
            "[1600] train_loss: 2.201 train_acc: 0.249    val_loss: 2.214 val_acc : 0.236\n",
            "[1700] train_loss: 2.176 train_acc: 0.253    val_loss: 2.152 val_acc : 0.247\n",
            "[1800] train_loss: 2.180 train_acc: 0.265    val_loss: 2.148 val_acc : 0.276\n",
            "[1900] train_loss: 2.191 train_acc: 0.252    val_loss: 2.183 val_acc : 0.268\n",
            "[2000] train_loss: 2.197 train_acc: 0.250    val_loss: 2.177 val_acc : 0.251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aX8WmwsKV2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}